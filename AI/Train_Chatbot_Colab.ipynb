{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip -q install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --pre pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\\n\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc \n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "import os\n",
    "import socket\n",
    "from pprint import pformat\n",
    "from datetime import datetime\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer, AdamW, WEIGHTS_NAME, CONFIG_NAME\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.handlers import ModelCheckpoint, global_step_from_engine\n",
    "from ignite.metrics import Accuracy, Loss, MetricsLambda, RunningAverage\n",
    "from ignite.contrib.handlers import ProgressBar, PiecewiseLinear\n",
    "from ignite.contrib.handlers.tensorboard_logger import TensorboardLogger, OutputHandler, OptimizerParamsHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"[cls]\", \"<pad>\"]\n",
    "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<bos>', 'eos_token': '<eos>', 'cls_token': '[CLS]', 'pad_token': '<pad>', \n",
    "                         'additional_special_tokens': ['<speaker1>', '<speaker2>', 'anythingElse', 'none', 'greet', 'nothingElse', 'listProjects', 'helpYou', 'listTasks', 'thanks', 'onIt'],\n",
    "                        }\n",
    "\n",
    "orig_num_tokens = len(tokenizer.encoder)\n",
    "num_added_tokens = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN) # doesn't add if they are already there\n",
    "if num_added_tokens > 0:\n",
    "    model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=6.25e-5, correct_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(obj):\n",
    "    if isinstance(obj, str):\n",
    "        return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
    "    if isinstance(obj, dict):\n",
    "        return dict((n, tokenize(o)) for n, o in obj.items())\n",
    "    return list(tokenize(o) for o in obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(history, reply, tokenizer, lm_labels=True, with_eos=True):\n",
    "    \"\"\" Build a sequence of input from 2 segments: history and last reply. \"\"\"\n",
    "    bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-2])\n",
    "    sequence = [[bos]] + history + [reply + ([eos] if with_eos else [])]\n",
    "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
    "    instance = {}\n",
    "    instance[\"input_ids\"] = list(chain(*sequence))\n",
    "    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
    "    #instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
    "    instance[\"lm_labels\"] = [-100] * len(instance[\"input_ids\"])\n",
    "    #if lm_labels:\n",
    "        #instance[\"lm_labels\"] = ([-100] * sum(len(s) for s in sequence[:-1])) + [-100] + sequence[-1][1:]\n",
    "    instance[\"lm_labels\"] = ([-100] * sum(len(s) for s in sequence[:-1])) + [-100] + sequence[-1][1:]\n",
    "    return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_dataset(dataset, padding=0):\n",
    "    PADDED_INPUTS = [\"input_ids\", \"lm_labels\", \"token_type_ids\"]\n",
    "    max_l = max(len(x) for x in dataset[\"input_ids\"])\n",
    "    for name in PADDED_INPUTS:\n",
    "        dataset[name] = [x + [padding if name != \"lm_labels\" else -100] * (max_l - len(x)) for x in dataset[name]]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/content/drive/My Drive/Chatbot/fullTrain.json') as t:\n",
    "    train = json.load(t)\n",
    "\n",
    "with open('/content/drive/My Drive/Chatbot/fullTest.json') as t:\n",
    "    test = json.load(t)\n",
    "    \n",
    "tokenTrain = tokenize(train)\n",
    "\n",
    "tokenTest = tokenize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['history'][4], train['history'][1][-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\"train\": defaultdict(list), \"test\": defaultdict(list)}\n",
    "\n",
    "for i in range(len(tokenTrain['history'])):\n",
    "    \n",
    "    #instance_train = build_inputs([tokenTrain[\"lastInput\"][i]], tokenTrain[\"reply\"][i], tokenizer, True)\n",
    "    #instance_test = build_inputs([tokenTest[\"lastInput\"][i]], tokenTest[\"reply\"][i], tokenizer, True)\n",
    "    \n",
    "    #### Only the last 3 occurences (the db has onlt the last 4 so...)\n",
    "    instance_train = build_inputs(tokenTrain[\"history\"][i][-3:], tokenTrain[\"reply\"][i], tokenizer, True)\n",
    "    instance_test = build_inputs(tokenTest[\"history\"][i][-3:], tokenTest[\"reply\"][i], tokenizer, True)\n",
    "    \n",
    "    for input_name, input_array in instance_train.items():\n",
    "        datasets[\"train\"][input_name].append(input_array)\n",
    "    \n",
    "    for input_name, input_array in instance_test.items():\n",
    "        datasets[\"test\"][input_name].append(input_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_INPUTS = [\"input_ids\", \"lm_labels\", \"token_type_ids\"]\n",
    "tensor_datasets = {\"train\": [], \"test\": []}\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    dataset = pad_dataset(dataset, padding=tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1]))\n",
    "    for input_name in MODEL_INPUTS:\n",
    "        tensor = torch.tensor(dataset[input_name])\n",
    "        tensor_datasets[dataset_name].append(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = TensorDataset(*tensor_datasets[\"train\"]), TensorDataset(*tensor_datasets[\"test\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, sampler=None, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, sampler=None, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_logdir(model_name: str):\n",
    "    \"\"\"Create unique path to save results and checkpoints, e.g. runs/Sep22_19-45-59_gpu-7_gpt2\"\"\"\n",
    "    # Code copied from ignite repo\n",
    "    current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "    logdir = os.path.join(\n",
    "        '/content/drive/My Drive/Chatbot/runs', current_time + '_' + socket.gethostname() + '_' + model_name)\n",
    "    return logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    def update(engine, batch):\n",
    "        model.train()\n",
    "        batch = tuple(input_tensor.to(device) for input_tensor in batch)\n",
    "        input_ids, lm_labels, token_type_ids = batch\n",
    "        lm_loss, *_ = model(input_ids, token_type_ids=token_type_ids, labels=lm_labels)\n",
    "        loss = (lm_loss * 1.0 )/ 8\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        if engine.state.iteration % 8 == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        return loss.item()\n",
    "    \n",
    "    trainer = Engine(update)\n",
    "    \n",
    "    # Evaluation function and evaluator (evaluator output is the input of the metrics)\n",
    "    def inference(engine, batch):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch = tuple(input_tensor.to(device) for input_tensor in batch)\n",
    "            input_ids, lm_labels, token_type_ids = batch\n",
    "            #logger.info(tokenizer.decode(input_ids[0, -1, :].tolist()))\n",
    "            # if we dont send labels to model, it doesnt return losses\n",
    "            lm_logits, *_ = model(input_ids, token_type_ids=token_type_ids)\n",
    "            \n",
    "            lm_logits_flat_shifted = lm_logits[:, :-1].contiguous().view(-1, lm_logits.size(-1))\n",
    "            lm_labels_flat_shifted = lm_labels[:, 1:].contiguous().view(-1)\n",
    "            return (lm_logits_flat_shifted), (lm_labels_flat_shifted)\n",
    "        \n",
    "    evaluator = Engine(inference)\n",
    "    \n",
    "    n_epochs = 2\n",
    "    eval_before_start = True\n",
    "    \n",
    "    # Attach evaluation to trainer: we evaluate when we start the training and at the end of each epoch\n",
    "    trainer.add_event_handler(Events.EPOCH_COMPLETED, lambda _: evaluator.run(test_loader))\n",
    "    if n_epochs < 1:\n",
    "        trainer.add_event_handler(Events.COMPLETED, lambda _: evaluator.run(test_loader))\n",
    "    if eval_before_start:\n",
    "        trainer.add_event_handler(Events.STARTED, lambda _: evaluator.run(test_loader))\n",
    "    \n",
    "    # Linearly decrease the learning rate from lr to zero\n",
    "    scheduler = PiecewiseLinear(optimizer, \"lr\", [(0, 6.25e-5), (n_epochs * len(train_loader), 0.0)])\n",
    "    trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
    "    \n",
    "        \n",
    "    # Prepare metrics - note how we compute distributed metrics\n",
    "    RunningAverage(output_transform=lambda x: x).attach(trainer, \"loss\")\n",
    "    metrics = {\"nll\": Loss(torch.nn.CrossEntropyLoss(ignore_index=-100), output_transform=lambda x: (x[0], x[1])),\n",
    "               \"accuracy\": Accuracy(output_transform=lambda x: (x[0], x[1]))}\n",
    "    #Accuracy(output_transform=lambda x: (x[0][1], x[1][1]) Accuracy(thresholded_output_transform)\n",
    "    metrics.update({\"average_nll\": metrics[\"nll\"],\n",
    "                    \"average_accuracy\": metrics[\"accuracy\"]})\n",
    "    \n",
    "    metrics[\"average_ppl\"] = MetricsLambda(math.exp, metrics[\"average_nll\"])\n",
    "    for name, metric in metrics.items():\n",
    "        metric.attach(evaluator, name)\n",
    "    \n",
    "    local_rank = -1\n",
    "    # On the main process: add progress bar, tensorboard, checkpoints and save model, configuration and tokenizer before we start to train\n",
    "    if local_rank in [-1, 0]:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        pbar = ProgressBar(persist=True)\n",
    "        pbar.attach(trainer, metric_names=[\"loss\"])\n",
    "        evaluator.add_event_handler(Events.COMPLETED, lambda _: pbar.log_message(\"Validation: %s\" % pformat(evaluator.state.metrics)))\n",
    "\n",
    "        log_dir = make_logdir(\"gpt2\")\n",
    "        tb_logger = TensorboardLogger(log_dir)\n",
    "        \n",
    "        tb_logger.attach(trainer, log_handler=OutputHandler(tag=\"training\", metric_names=[\"loss\"]), event_name=Events.ITERATION_COMPLETED)\n",
    "        tb_logger.attach(trainer, log_handler=OptimizerParamsHandler(optimizer), event_name=Events.ITERATION_STARTED)\n",
    "        tb_logger.attach(evaluator, log_handler=OutputHandler(tag=\"validation\", metric_names=list(metrics.keys()), global_step_transform=global_step_from_engine(trainer)), event_name=Events.EPOCH_COMPLETED)\n",
    "\n",
    "        checkpoint_handler = ModelCheckpoint(log_dir, 'checkpoint', save_interval=1, n_saved=3)\n",
    "        trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {'mymodel': getattr(model, 'module', model)})  # \"getattr\" takes care of distributed encapsulation\n",
    "\n",
    "        #torch.save(args, log_dir + '/model_training_args.bin')\n",
    "        getattr(model, 'module', model).config.to_json_file(os.path.join(log_dir, CONFIG_NAME))\n",
    "        tokenizer.save_pretrained(log_dir)\n",
    "    \n",
    "    # Run the training\n",
    "    trainer.run(train_loader, max_epochs=n_epochs)\n",
    "    \n",
    "    # On the main process: close tensorboard logger and rename the last checkpoint (for easy re-loading with OpenAIGPTModel.from_pretrained method)\n",
    "    if local_rank in [-1, 0] and n_epochs > 0:\n",
    "        os.rename(os.path.join(log_dir, checkpoint_handler._saved[-1][1]), os.path.join(log_dir, WEIGHTS_NAME))  # TODO: PR in ignite to have better access to saved file paths (cleaner)\n",
    "        tb_logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
