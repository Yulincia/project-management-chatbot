{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pytorch-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "import logging\n",
    "\n",
    "from transformers import ElectraTokenizer, ElectraModel\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, LayerNorm\n",
    "from torchnlp.metrics import get_accuracy, get_token_accuracy\n",
    "from pytorch_lightning.metrics.functional import f1_score\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RasaIntentEntityDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, markdown_lines: List[str], tokenizer, seq_len=128,):\n",
    "        self.intent_dict = {}\n",
    "        self.entity_dict = {}\n",
    "        self.entity_dict[\"O\"] = 0  # using BIO tagging\n",
    "\n",
    "        self.dataset = []\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        intent_value_list = []\n",
    "        entity_type_list = []\n",
    "\n",
    "        current_intent_focus = \"\"\n",
    "\n",
    "        text_list = []\n",
    "\n",
    "        for line in tqdm(markdown_lines,desc=\"Organizing Intent & Entity dictionary in NLU markdown file ...\",):\n",
    "            if len(line.strip()) < 2:\n",
    "                current_intent_focus = \"\"\n",
    "                continue\n",
    "\n",
    "            if \"## \" in line:\n",
    "                if \"intent:\" in line:\n",
    "                    intent_value_list.append(line.split(\":\")[1].strip())\n",
    "                    current_intent_focus = line.split(\":\")[1].strip()\n",
    "                else:\n",
    "                    current_intent_focus = \"\"\n",
    "\n",
    "            else:\n",
    "                if current_intent_focus != \"\":\n",
    "                    text = line[2:].strip().lower()\n",
    "\n",
    "                    for type_str in re.finditer(r\"\\([a-zA-Z_1-2]+\\)\", text):\n",
    "                        entity_type = (text[type_str.start() + 1 : type_str.end() - 1].replace(\"(\", \"\").replace(\")\", \"\"))\n",
    "                        entity_type_list.append(entity_type)\n",
    "\n",
    "                    text = re.sub(r\"\\([a-zA-Z_1-2]+\\)\", \"\", text)  # remove (...) str\n",
    "                    text = text.replace(\"[\", \"\").replace(\"]\", \"\")  # remove '[',']' special char\n",
    "\n",
    "                    if len(text) > 0:\n",
    "                        text_list.append(text.strip())\n",
    "\n",
    "\n",
    "        #dataset tokenizer setting\n",
    "        if \"ElectraTokenizer\" in str(type(tokenizer)):\n",
    "            self.tokenizer = tokenizer\n",
    "            self.pad_token_id = 0\n",
    "            self.unk_token_id = 1\n",
    "            self.eos_token_id = 3 #[SEP] token\n",
    "            self.bos_token_id = 2 #[CLS] token\n",
    "\n",
    "        else:\n",
    "            raise ValueError('not supported tokenizer type')\n",
    "\n",
    "        intent_value_list = sorted(intent_value_list)\n",
    "        for intent_value in intent_value_list:\n",
    "            if intent_value not in self.intent_dict.keys():\n",
    "                self.intent_dict[intent_value] = len(self.intent_dict)\n",
    "\n",
    "        entity_type_list = sorted(entity_type_list)\n",
    "        for entity_type in entity_type_list:\n",
    "            if entity_type + '_B' not in self.entity_dict.keys():\n",
    "                self.entity_dict[str(entity_type) + '_B'] = len(self.entity_dict)\n",
    "            if entity_type + '_I' not in self.entity_dict.keys():\n",
    "                self.entity_dict[str(entity_type) + '_I'] = len(self.entity_dict)\n",
    "\n",
    "        current_intent_focus = \"\"\n",
    "\n",
    "        for line in tqdm(markdown_lines, desc=\"Extracting Intent & Entity in NLU markdown files...\",):\n",
    "            if len(line.strip()) < 2:\n",
    "                current_intent_focus = \"\"\n",
    "                continue\n",
    "\n",
    "            if \"## \" in line:\n",
    "                if \"intent:\" in line:\n",
    "                    current_intent_focus = line.split(\":\")[1].strip()\n",
    "                else:\n",
    "                    current_intent_focus = \"\"\n",
    "            else:\n",
    "                if current_intent_focus != \"\":  # intent & entity sentence occur case\n",
    "                    text = line[2:].strip().lower()\n",
    "\n",
    "                    entity_value_list = []\n",
    "                    for value in re.finditer(r\"\\[(.*?)\\]\", text):\n",
    "                        entity_value_list.append(text[value.start() + 1 : value.end() - 1].replace(\"[\",\"\").replace(\"]\",\"\"))\n",
    "\n",
    "                    entity_type_list = []\n",
    "                    for type_str in re.finditer(r\"\\([a-zA-Z_1-2]+\\)\", text):\n",
    "                        entity_type = (text[type_str.start() + 1 : type_str.end() - 1].replace(\"(\",\"\").replace(\")\",\"\"))\n",
    "                        entity_type_list.append(entity_type)\n",
    "\n",
    "                    text = re.sub(r\"\\([a-zA-Z_1-2]+\\)\", \"\", text)  # remove (...) str\n",
    "                    text = text.replace(\"[\", \"\").replace(\"]\", \"\")  # remove '[',']' special char\n",
    "\n",
    "                    if len(text) > 0:\n",
    "                        each_data_dict = {}\n",
    "                        each_data_dict[\"text\"] = text.strip()\n",
    "                        each_data_dict[\"intent\"] = current_intent_focus\n",
    "                        each_data_dict[\"intent_idx\"] = self.intent_dict[current_intent_focus]\n",
    "                        each_data_dict[\"entities\"] = []\n",
    "\n",
    "                        for value, type_str in zip(entity_value_list, entity_type_list):\n",
    "                            for entity in re.finditer(value, text):\n",
    "                                entity_tokens = self.tokenize(value)\n",
    "\n",
    "                                for i, entity_token in enumerate(entity_tokens):\n",
    "                                    if i == 0:\n",
    "                                        BIO_type_str = type_str + '_B'\n",
    "                                    else:\n",
    "                                        BIO_type_str = type_str + '_I'\n",
    "\n",
    "                                    each_data_dict[\"entities\"].append(\n",
    "                                        {\n",
    "                                            \"start\": text.find(entity_token, entity.start(), entity.end()),\n",
    "                                            \"end\": text.find(entity_token, entity.start(), entity.end()) + len(entity_token),\n",
    "                                            \"entity\": type_str,\n",
    "                                            \"value\": entity_token,\n",
    "                                            \"entity_idx\": self.entity_dict[BIO_type_str],\n",
    "                                        }\n",
    "                                    )\n",
    "\n",
    "\n",
    "                        self.dataset.append(each_data_dict)\n",
    "\n",
    "        \n",
    "        print(f\"Intents: {self.intent_dict}\")\n",
    "        print(f\"Entities: {self.entity_dict}\")\n",
    "\n",
    "    def tokenize(self, text: str, skip_special_char=True):\n",
    "        if \"ElectraTokenizer\" in str(type(self.tokenizer)):\n",
    "            if skip_special_char:\n",
    "                return self.tokenizer.tokenize(text)\n",
    "            else:\n",
    "                return [token.replace('#','') for token in self.tokenizer.tokenize(text)]\n",
    "        else:\n",
    "            raise ValueError('not supported tokenizer type')\n",
    "            \n",
    "    def encode(self, text: str, padding: bool = True, return_tensor: bool = True):\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        if type(tokens) == list:\n",
    "            tokens = torch.tensor(tokens).long()\n",
    "        else:\n",
    "            tokens = tokens.long()\n",
    "\n",
    "        if padding:\n",
    "            if len(tokens) >= self.seq_len:\n",
    "                tokens = tokens[: self.seq_len]\n",
    "            else:\n",
    "                pad_tensor = torch.tensor([self.pad_token_id] * (self.seq_len - len(tokens)))\n",
    "            \n",
    "                tokens = torch.cat((tokens, pad_tensor), 0)\n",
    "\n",
    "        if return_tensor:\n",
    "            return tokens\n",
    "        else:\n",
    "            return tokens.numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.encode(self.dataset[idx][\"text\"])\n",
    "\n",
    "        intent_idx = torch.tensor([self.dataset[idx][\"intent_idx\"]])\n",
    "\n",
    "        entity_idx = np.array(self.seq_len * [0]) # O tag indicate 0(zero)\n",
    "\n",
    "        for entity_info in self.dataset[idx][\"entities\"]:\n",
    "            if \"ElectraTokenizer\" in str(type(self.tokenizer)):\n",
    "                ##check whether entity value is include in splitted token\n",
    "                for token_seq, token_value in enumerate(tokens):\n",
    "                    # Consider [CLS](bos) token\n",
    "                    if token_seq == 0:\n",
    "                        continue\n",
    "\n",
    "                    for entity_seq, entity_info in enumerate(self.dataset[idx][\"entities\"]):\n",
    "                        if (self.tokenizer.convert_ids_to_tokens([token_value.item()])[0] in entity_info[\"value\"]):\n",
    "                            entity_idx[token_seq] = entity_info[\"entity_idx\"]\n",
    "                            break\n",
    "\n",
    "        entity_idx = torch.from_numpy(entity_idx)\n",
    "\n",
    "        return tokens, intent_idx, entity_idx, self.dataset[idx][\"text\"]\n",
    "\n",
    "    def get_intent_idx(self):\n",
    "        return self.intent_dict\n",
    "\n",
    "    def get_entity_idx(self):\n",
    "        return self.entity_dict\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.tokenizer.vocab_size\n",
    "\n",
    "    def get_seq_len(self):\n",
    "        return self.seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: None,\n",
    "        vocab_size: int,\n",
    "        seq_len: int,\n",
    "        intent_class_num: int,\n",
    "        entity_class_num: int,\n",
    "        d_model=512,\n",
    "        nhead=8,\n",
    "        num_encoder_layers=6,\n",
    "        dim_feedforward=2048,\n",
    "        dropout=0.1,\n",
    "        activation=\"relu\",\n",
    "        pad_token_id: int = 0,\n",
    "    ):\n",
    "        super(EmbeddingTransformer, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.seq_len = seq_len\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "        if backbone is None:\n",
    "            self.encoder = nn.TransformerEncoder(\n",
    "                TransformerEncoderLayer(\n",
    "                    d_model, nhead, dim_feedforward, dropout, activation\n",
    "                ),\n",
    "                num_encoder_layers,\n",
    "                LayerNorm(d_model),\n",
    "            )\n",
    "        else:  # pre-defined model architecture use\n",
    "            if backbone == \"electra\":\n",
    "                self.encoder = ElectraModel.from_pretrained(\"google/electra-small-discriminator\")\n",
    "\n",
    "            d_model = self.encoder.config.hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(self.seq_len, d_model)\n",
    "\n",
    "        self.intent_feature = nn.Linear(d_model, intent_class_num)\n",
    "        self.entity_feature = nn.Linear(d_model, entity_class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.backbone in [\"electra\"]:\n",
    "            feature = self.encoder(x)\n",
    "\n",
    "            if type(feature) == tuple:\n",
    "                feature = feature[0]  # last_hidden_state (N,S,E)\n",
    "\n",
    "            # first token in sequence used to intent classification\n",
    "            intent_feature = self.intent_feature(feature[:, 0, :]) # (N,E) -> (N,i_C)\n",
    "\n",
    "            # other tokens in sequence used to entity classification\n",
    "            entity_feature = self.entity_feature(feature[:, :, :]) # (N,S,E) -> (N,S,e_C)\n",
    "\n",
    "            return intent_feature, entity_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualIntentEntityTransformer(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hparams = hparams\n",
    "        if type(self.hparams) == dict:\n",
    "            self.hparams = Namespace(**self.hparams)\n",
    "\n",
    "        self.dataset = RasaIntentEntityDataset(\n",
    "            markdown_lines=self.hparams.nlu_data,\n",
    "            tokenizer=self.hparams.tokenizer,\n",
    "        )\n",
    "\n",
    "        self.model = EmbeddingTransformer(\n",
    "            backbone=self.hparams.backbone,\n",
    "            vocab_size=self.dataset.get_vocab_size(),\n",
    "            seq_len=self.dataset.get_seq_len(),\n",
    "            intent_class_num=len(self.dataset.get_intent_idx()),\n",
    "            entity_class_num=len(self.dataset.get_entity_idx()),\n",
    "            d_model=self.hparams.d_model,\n",
    "            num_encoder_layers=self.hparams.num_encoder_layers,\n",
    "            pad_token_id=self.dataset.pad_token_id\n",
    "        )\n",
    "\n",
    "        self.train_ratio = self.hparams.train_ratio\n",
    "        self.batch_size = self.hparams.batch_size\n",
    "        self.optimizer = self.hparams.optimizer\n",
    "        self.intent_optimizer_lr = self.hparams.intent_optimizer_lr\n",
    "        self.entity_optimizer_lr = self.hparams.entity_optimizer_lr\n",
    "\n",
    "        self.intent_loss_fn = nn.CrossEntropyLoss()\n",
    "        # reduce O tag class weight to figure out entity imbalance distribution\n",
    "        self.entity_loss_fn = nn.CrossEntropyLoss(weight=torch.Tensor([0.1] + [1.0] * (len(self.dataset.get_entity_idx()) - 1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        train_length = int(len(self.dataset) * self.train_ratio)\n",
    "\n",
    "        self.train_dataset, self.val_dataset = random_split(\n",
    "            self.dataset, [train_length, len(self.dataset) - train_length],\n",
    "        )\n",
    "\n",
    "        self.hparams.intent_label = self.get_intent_label()\n",
    "        self.hparams.entity_label = self.get_entity_label()\n",
    "    \n",
    "    def get_intent_label(self):\n",
    "        self.intent_dict = {}\n",
    "        for k, v in self.dataset.intent_dict.items():\n",
    "            self.intent_dict[str(v)] = k\n",
    "        return self.intent_dict \n",
    "    \n",
    "    def get_entity_label(self):\n",
    "        self.entity_dict = {}\n",
    "        for k, v in self.dataset.entity_dict.items():\n",
    "            self.entity_dict[str(v)] = k\n",
    "        return self.entity_dict\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "        return val_loader\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        intent_optimizer = eval(\n",
    "            f\"{self.optimizer}(self.parameters(), lr={self.intent_optimizer_lr})\"\n",
    "        )\n",
    "        entity_optimizer = eval(\n",
    "            f\"{self.optimizer}(self.parameters(), lr={self.entity_optimizer_lr})\"\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            [intent_optimizer, entity_optimizer],\n",
    "            # [StepLR(intent_optimizer, step_size=1),StepLR(entity_optimizer, step_size=1),],\n",
    "            [\n",
    "                ReduceLROnPlateau(intent_optimizer, patience=1),\n",
    "                ReduceLROnPlateau(entity_optimizer, patience=1),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        self.model.train()\n",
    "\n",
    "        tokens, intent_idx, entity_idx, text = batch\n",
    "        intent_pred, entity_pred = self.forward(tokens)\n",
    "\n",
    "        intent_acc = get_accuracy(intent_pred.argmax(1).cpu(), intent_idx.cpu())[0]\n",
    "        #entity_acc = get_token_accuracy(entity_pred.argmax(2), entity_idx, ignore_index=self.dataset.pad_token_id)[0]\n",
    "\n",
    "        tensorboard_logs = {\n",
    "            \"train/intent/acc\": intent_acc,\n",
    "            #\"train/entity/acc\": entity_acc,\n",
    "        }\n",
    "\n",
    "        if optimizer_idx == 0:\n",
    "            intent_loss = self.intent_loss_fn(intent_pred, intent_idx.squeeze(1))\n",
    "            tensorboard_logs[\"train/intent/loss\"] = intent_loss\n",
    "\n",
    "            return {\n",
    "                \"loss\": intent_loss,\n",
    "                \"log\": tensorboard_logs,\n",
    "            }\n",
    "\n",
    "        if optimizer_idx == 1:\n",
    "            entity_loss = self.entity_loss_fn(entity_pred.transpose(1, 2), entity_idx.long(),)\n",
    "            tensorboard_logs[\"train/entity/loss\"] = entity_loss\n",
    "\n",
    "            return {\n",
    "                \"loss\": entity_loss,\n",
    "                \"log\": tensorboard_logs,\n",
    "            }\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.model.eval()\n",
    "\n",
    "        tokens, intent_idx, entity_idx, text = batch\n",
    "        intent_pred, entity_pred = self.forward(tokens)\n",
    "        \n",
    "\n",
    "        intent_acc = get_accuracy(intent_pred.argmax(1).cpu(), intent_idx.cpu())[0]\n",
    "        #entity_acc = get_token_accuracy(entity_pred.argmax(2), entity_idx, ignore_index=self.dataset.pad_token_id)[0]\n",
    "        intent_f1 = f1_score(intent_pred.argmax(1), intent_idx)\n",
    "\n",
    "        intent_loss = self.intent_loss_fn(intent_pred, intent_idx.squeeze(1))\n",
    "        entity_loss = self.entity_loss_fn(entity_pred.transpose(1, 2), entity_idx.long(),)\n",
    "\n",
    "        return {\n",
    "            \"val_intent_acc\": torch.Tensor([intent_acc]),\n",
    "            #\"val_entity_acc\": torch.Tensor([entity_acc]),\n",
    "            \"val_intent_f1\": intent_f1,\n",
    "            \"val_intent_loss\": intent_loss,\n",
    "            \"val_entity_loss\": entity_loss,\n",
    "            \"val_loss\": intent_loss + entity_loss,\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        avg_intent_acc = torch.stack([x[\"val_intent_acc\"] for x in outputs]).mean()\n",
    "        #avg_entity_acc = torch.stack([x[\"val_entity_acc\"] for x in outputs]).mean()\n",
    "        avg_intent_f1 = torch.stack([x[\"val_intent_f1\"] for x in outputs]).mean()\n",
    "\n",
    "        tensorboard_logs = {\n",
    "            \"val/loss\": avg_loss,\n",
    "            \"val/intent_acc\": avg_intent_acc,\n",
    "            #\"val/entity_acc\": avg_entity_acc,\n",
    "            \"val/intent_f1\": avg_intent_f1,\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"val_loss\": avg_loss,\n",
    "            \"val_intent_f1\": avg_intent_f1,\n",
    "            #\"val_entity_acc\": avg_entity_acc,\n",
    "            \"log\": tensorboard_logs,\n",
    "            \"progress_bar\": tensorboard_logs,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "intent_dict = {}\n",
    "entity_dict = {}\n",
    "\n",
    "class Inferencer:\n",
    "    def __init__(self, checkpoint_path: str):\n",
    "        self.model = DualIntentEntityTransformer.load_from_checkpoint(checkpoint_path)\n",
    "        self.model.model.eval()\n",
    "\n",
    "        self.intent_dict = {}\n",
    "        for k, v in self.model.dataset.intent_dict.items():\n",
    "            self.intent_dict[v] = k # str key -> int key\n",
    "\n",
    "        self.entity_dict = {}\n",
    "        for k, v in self.model.dataset.entity_dict.items():\n",
    "            self.entity_dict[v] = k # str key -> int key\n",
    "\n",
    "        logging.info(\"intent dictionary\")\n",
    "        logging.info(self.intent_dict)\n",
    "        print()\n",
    "\n",
    "        logging.info(\"entity dictionary\")\n",
    "        logging.info(self.entity_dict)\n",
    "\n",
    "    def is_same_entity(self, i, j):\n",
    "        # check whether XXX_B, XXX_I tag are same \n",
    "        return self.entity_dict[i][:self.entity_dict[i].rfind('_')].strip() == self.entity_dict[j][:self.entity_dict[j].rfind('_')].strip()\n",
    "\n",
    "    def inference(self, text: str, intent_topk=5):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\n",
    "                \"model is not loaded, first call load_model(checkpoint_path)\"\n",
    "            )\n",
    "\n",
    "        text = text.strip().lower()\n",
    "\n",
    "        # encode text to token_indices\n",
    "        tokens = self.model.dataset.encode(text)\n",
    "        intent_result, entity_result = self.model.forward(tokens.unsqueeze(0).cpu())\n",
    "\n",
    "        # mapping intent result\n",
    "        rank_values, rank_indicies = torch.topk(\n",
    "            nn.Softmax(dim=1)(intent_result)[0], k=intent_topk\n",
    "        )\n",
    "        intent = {}\n",
    "        intent_ranking = []\n",
    "        for i, (value, index) in enumerate(\n",
    "            list(zip(rank_values.tolist(), rank_indicies.tolist()))\n",
    "        ):\n",
    "            intent_ranking.append(\n",
    "                {\"confidence\": value, \"name\": self.intent_dict[index]}\n",
    "            )\n",
    "\n",
    "            if i == 0:\n",
    "                intent[\"name\"] = self.intent_dict[index]\n",
    "                intent[\"confidence\"] = value\n",
    "\n",
    "        # mapping entity result\n",
    "        entities = []\n",
    "\n",
    "        # except first & last sequnce token whcih indicate BOS or [CLS] token & EOS or [SEP] token\n",
    "        _, entity_indices = torch.max((entity_result)[0][1:-1, :], dim=1)\n",
    "        start_idx = -1\n",
    "\n",
    "        #print ('tokens')\n",
    "        #print (tokens)\n",
    "        #print ('predicted entities')\n",
    "        #print (entity_indices)\n",
    "\n",
    "        entity_indices = entity_indices.tolist()[:len(text)]\n",
    "        start_token_position = -1\n",
    "\n",
    "        # except first sequnce token whcih indicate BOS or [CLS] token\n",
    "        if type(tokens) == torch.Tensor:\n",
    "            tokens = tokens.long().tolist()\n",
    "\n",
    "        for i, entity_idx_value in enumerate(entity_indices):\n",
    "            if entity_idx_value != 0 and start_token_position == -1:\n",
    "                start_token_position = i\n",
    "            elif start_token_position >= 0 and not self.is_same_entity(entity_indices[i-1],entity_indices[i]):\n",
    "                end_token_position = i - 1\n",
    "\n",
    "                #print ('start_token_position')\n",
    "                #print (start_token_position)\n",
    "                #print ('end_token_position')\n",
    "                #print (end_token_position)\n",
    "\n",
    "                # find start text position\n",
    "                token_idx = tokens[start_token_position + 1]\n",
    "                if \"ElectraTokenizer\" in str(\n",
    "                    type(self.model.dataset.tokenizer)\n",
    "                ):  # ElectraTokenizer\n",
    "                    token_value = self.model.dataset.tokenizer.convert_ids_to_tokens([token_idx])[0].replace(\"#\", \"\")\n",
    "\n",
    "                if len(token_value.strip()) == 0:\n",
    "                    start_token_position = -1\n",
    "                    continue\n",
    "                  \n",
    "                start_position = text.find(token_value.strip())\n",
    "\n",
    "                # find end text position\n",
    "                token_idx = tokens[end_token_position + 1]\n",
    "                if \"ElectraTokenizer\" in str(\n",
    "                    type(self.model.dataset.tokenizer)\n",
    "                ):  # ElectraTokenizer\n",
    "                    token_value = self.model.dataset.tokenizer.convert_ids_to_tokens([token_idx])[0].replace(\"#\", \"\")\n",
    "\n",
    "                end_position = text.find(token_value.strip(), start_position) + len(token_value.strip())\n",
    "\n",
    "                if self.entity_dict[entity_indices[i-1]] != \"O\": # ignore 'O' tag\n",
    "                    entities.append(\n",
    "                         {\n",
    "                            \"start\": start_position,\n",
    "                            \"end\": end_position,\n",
    "                            \"value\": text[start_position:end_position],\n",
    "                            \"entity\": self.entity_dict[entity_indices[i-1]][:self.entity_dict[entity_indices[i-1]].rfind('_')]\n",
    "                        }\n",
    "                    )\n",
    "                      \n",
    "                    start_token_position = -1\n",
    "\n",
    "                if entity_idx_value == 0:\n",
    "                    start_token_position = -1\n",
    "\n",
    "        result = {\n",
    "            \"text\": text,\n",
    "            \"intent\": intent,\n",
    "            \"intent_ranking\": intent_ranking,\n",
    "            \"entities\": entities,\n",
    "        }\n",
    "\n",
    "        # print (result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Organizing Intent & Entity dictionary in NLU markdown file ...: 100%|██████████| 991/991 [00:00<00:00, 195170.93it/s]\n",
      "Extracting Intent & Entity in NLU markdown files...: 100%|██████████| 991/991 [00:00<00:00, 18692.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intents: {'affirmative': 0, 'anythingElse': 1, 'bye': 2, 'greet': 3, 'listProjects': 4, 'listTasks': 5, 'negative': 6, 'nothingElse': 7}\n",
      "Entities: {'O': 0, 'priority_B': 1, 'priority_I': 2, 'state_B': 3, 'state_I': 4}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inferencer = Inferencer(\"lightning_logs/version_35/checkpoints/epoch=19.ckpt\")\n",
    "intent_entity = inferencer.inference(\"list of tasks finished with priority 0\", intent_topk=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'list of tasks finished with priority 0',\n",
       " 'intent': {'name': 'listTasks', 'confidence': 0.8964384198188782},\n",
       " 'intent_ranking': [{'confidence': 0.8964384198188782, 'name': 'listTasks'},\n",
       "  {'confidence': 0.05061880871653557, 'name': 'listProjects'},\n",
       "  {'confidence': 0.01707865111529827, 'name': 'negative'},\n",
       "  {'confidence': 0.008030795492231846, 'name': 'anythingElse'},\n",
       "  {'confidence': 0.0074590472504496574, 'name': 'greet'}],\n",
       " 'entities': [{'start': 14, 'end': 22, 'value': 'finished', 'entity': 'state'},\n",
       "  {'start': 37, 'end': 38, 'value': '0', 'entity': 'priority'}]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intent:  listTasks\n",
      "entity:  state     value:  finished\n",
      "entity:  priority     value:  0\n"
     ]
    }
   ],
   "source": [
    "intent = intent_entity['intent']['name']\n",
    "entities = intent_entity['entities']\n",
    "print('intent: ',intent)\n",
    "if len(intent_entity['entities'])>0:\n",
    "    for i in range(len(intent_entity['entities'])):\n",
    "        print('entity: ',intent_entity['entities'][i]['entity'], '    value: ', intent_entity['entities'][i]['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
