{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pytorch-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "from transformers import ElectraTokenizer, ElectraModel\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from typing import List, Dict, Sequence\n",
    "from sklearn import metrics\n",
    "\n",
    "import glob\n",
    "from pytorch_lightning.callbacks.base import Callback\n",
    "\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, LayerNorm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from argparse import Namespace\n",
    "\n",
    "from torchnlp.metrics import get_accuracy, get_token_accuracy\n",
    "from pytorch_lightning.metrics.functional import f1_score\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RasaIntentEntityDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, markdown_lines: List[str], tokenizer, seq_len=128,):\n",
    "        self.intent_dict = {}\n",
    "        self.entity_dict = {}\n",
    "        self.entity_dict[\"O\"] = 0  # using BIO tagging\n",
    "\n",
    "        self.dataset = []\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        intent_value_list = []\n",
    "        entity_type_list = []\n",
    "\n",
    "        current_intent_focus = \"\"\n",
    "\n",
    "        text_list = []\n",
    "\n",
    "        for line in tqdm(markdown_lines,desc=\"Organizing Intent & Entity dictionary in NLU markdown file ...\",):\n",
    "            if len(line.strip()) < 2:\n",
    "                current_intent_focus = \"\"\n",
    "                continue\n",
    "\n",
    "            if \"## \" in line:\n",
    "                if \"intent:\" in line:\n",
    "                    intent_value_list.append(line.split(\":\")[1].strip())\n",
    "                    current_intent_focus = line.split(\":\")[1].strip()\n",
    "                else:\n",
    "                    current_intent_focus = \"\"\n",
    "\n",
    "            else:\n",
    "                if current_intent_focus != \"\":\n",
    "                    text = line[2:].strip().lower()\n",
    "\n",
    "                    for type_str in re.finditer(r\"\\([a-zA-Z_1-2]+\\)\", text):\n",
    "                        entity_type = (text[type_str.start() + 1 : type_str.end() - 1].replace(\"(\", \"\").replace(\")\", \"\"))\n",
    "                        entity_type_list.append(entity_type)\n",
    "\n",
    "                    text = re.sub(r\"\\([a-zA-Z_1-2]+\\)\", \"\", text)  # remove (...) str\n",
    "                    text = text.replace(\"[\", \"\").replace(\"]\", \"\")  # remove '[',']' special char\n",
    "\n",
    "                    if len(text) > 0:\n",
    "                        text_list.append(text.strip())\n",
    "\n",
    "\n",
    "        #dataset tokenizer setting\n",
    "        if \"ElectraTokenizer\" in str(type(tokenizer)):\n",
    "            self.tokenizer = tokenizer\n",
    "            self.pad_token_id = 0\n",
    "            self.unk_token_id = 1\n",
    "            self.eos_token_id = 3 #[SEP] token\n",
    "            self.bos_token_id = 2 #[CLS] token\n",
    "\n",
    "        else:\n",
    "            raise ValueError('not supported tokenizer type')\n",
    "\n",
    "        intent_value_list = sorted(intent_value_list)\n",
    "        for intent_value in intent_value_list:\n",
    "            if intent_value not in self.intent_dict.keys():\n",
    "                self.intent_dict[intent_value] = len(self.intent_dict)\n",
    "\n",
    "        entity_type_list = sorted(entity_type_list)\n",
    "        for entity_type in entity_type_list:\n",
    "            if entity_type + '_B' not in self.entity_dict.keys():\n",
    "                self.entity_dict[str(entity_type) + '_B'] = len(self.entity_dict)\n",
    "            if entity_type + '_I' not in self.entity_dict.keys():\n",
    "                self.entity_dict[str(entity_type) + '_I'] = len(self.entity_dict)\n",
    "\n",
    "        current_intent_focus = \"\"\n",
    "\n",
    "        for line in tqdm(markdown_lines, desc=\"Extracting Intent & Entity in NLU markdown files...\",):\n",
    "            if len(line.strip()) < 2:\n",
    "                current_intent_focus = \"\"\n",
    "                continue\n",
    "\n",
    "            if \"## \" in line:\n",
    "                if \"intent:\" in line:\n",
    "                    current_intent_focus = line.split(\":\")[1].strip()\n",
    "                else:\n",
    "                    current_intent_focus = \"\"\n",
    "            else:\n",
    "                if current_intent_focus != \"\":  # intent & entity sentence occur case\n",
    "                    text = line[2:].strip().lower()\n",
    "\n",
    "                    entity_value_list = []\n",
    "                    for value in re.finditer(r\"\\[(.*?)\\]\", text):\n",
    "                        entity_value_list.append(text[value.start() + 1 : value.end() - 1].replace(\"[\",\"\").replace(\"]\",\"\"))\n",
    "\n",
    "                    entity_type_list = []\n",
    "                    for type_str in re.finditer(r\"\\([a-zA-Z_1-2]+\\)\", text):\n",
    "                        entity_type = (text[type_str.start() + 1 : type_str.end() - 1].replace(\"(\",\"\").replace(\")\",\"\"))\n",
    "                        entity_type_list.append(entity_type)\n",
    "\n",
    "                    text = re.sub(r\"\\([a-zA-Z_1-2]+\\)\", \"\", text)  # remove (...) str\n",
    "                    text = text.replace(\"[\", \"\").replace(\"]\", \"\")  # remove '[',']' special char\n",
    "\n",
    "                    if len(text) > 0:\n",
    "                        each_data_dict = {}\n",
    "                        each_data_dict[\"text\"] = text.strip()\n",
    "                        each_data_dict[\"intent\"] = current_intent_focus\n",
    "                        each_data_dict[\"intent_idx\"] = self.intent_dict[current_intent_focus]\n",
    "                        each_data_dict[\"entities\"] = []\n",
    "\n",
    "                        for value, type_str in zip(entity_value_list, entity_type_list):\n",
    "                            for entity in re.finditer(value, text):\n",
    "                                entity_tokens = self.tokenize(value)\n",
    "\n",
    "                                for i, entity_token in enumerate(entity_tokens):\n",
    "                                    if i == 0:\n",
    "                                        BIO_type_str = type_str + '_B'\n",
    "                                    else:\n",
    "                                        BIO_type_str = type_str + '_I'\n",
    "\n",
    "                                    each_data_dict[\"entities\"].append(\n",
    "                                        {\n",
    "                                            \"start\": text.find(entity_token, entity.start(), entity.end()),\n",
    "                                            \"end\": text.find(entity_token, entity.start(), entity.end()) + len(entity_token),\n",
    "                                            \"entity\": type_str,\n",
    "                                            \"value\": entity_token,\n",
    "                                            \"entity_idx\": self.entity_dict[BIO_type_str],\n",
    "                                        }\n",
    "                                    )\n",
    "\n",
    "\n",
    "                        self.dataset.append(each_data_dict)\n",
    "\n",
    "        \n",
    "        print(f\"Intents: {self.intent_dict}\")\n",
    "        print(f\"Entities: {self.entity_dict}\")\n",
    "\n",
    "    def tokenize(self, text: str, skip_special_char=True):\n",
    "        if \"ElectraTokenizer\" in str(type(self.tokenizer)):\n",
    "            if skip_special_char:\n",
    "                return self.tokenizer.tokenize(text)\n",
    "            else:\n",
    "                return [token.replace('#','') for token in self.tokenizer.tokenize(text)]\n",
    "        else:\n",
    "            raise ValueError('not supported tokenizer type')\n",
    "            \n",
    "    def encode(self, text: str, padding: bool = True, return_tensor: bool = True):\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        if type(tokens) == list:\n",
    "            tokens = torch.tensor(tokens).long()\n",
    "        else:\n",
    "            tokens = tokens.long()\n",
    "\n",
    "        if padding:\n",
    "            if len(tokens) >= self.seq_len:\n",
    "                tokens = tokens[: self.seq_len]\n",
    "            else:\n",
    "                pad_tensor = torch.tensor([self.pad_token_id] * (self.seq_len - len(tokens)))\n",
    "            \n",
    "                tokens = torch.cat((tokens, pad_tensor), 0)\n",
    "\n",
    "        if return_tensor:\n",
    "            return tokens\n",
    "        else:\n",
    "            return tokens.numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.encode(self.dataset[idx][\"text\"])\n",
    "\n",
    "        intent_idx = torch.tensor([self.dataset[idx][\"intent_idx\"]])\n",
    "\n",
    "        entity_idx = np.array(self.seq_len * [0]) # O tag indicate 0(zero)\n",
    "\n",
    "        for entity_info in self.dataset[idx][\"entities\"]:\n",
    "            if \"ElectraTokenizer\" in str(type(self.tokenizer)):\n",
    "                ##check whether entity value is include in splitted token\n",
    "                for token_seq, token_value in enumerate(tokens):\n",
    "                    # Consider [CLS](bos) token\n",
    "                    if token_seq == 0:\n",
    "                        continue\n",
    "\n",
    "                    for entity_seq, entity_info in enumerate(self.dataset[idx][\"entities\"]):\n",
    "                        if (self.tokenizer.convert_ids_to_tokens([token_value.item()])[0] in entity_info[\"value\"]):\n",
    "                            entity_idx[token_seq] = entity_info[\"entity_idx\"]\n",
    "                            break\n",
    "\n",
    "        entity_idx = torch.from_numpy(entity_idx)\n",
    "\n",
    "        return tokens, intent_idx, entity_idx, self.dataset[idx][\"text\"]\n",
    "\n",
    "    def get_intent_idx(self):\n",
    "        return self.intent_dict\n",
    "\n",
    "    def get_entity_idx(self):\n",
    "        return self.entity_dict\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.tokenizer.vocab_size\n",
    "\n",
    "    def get_seq_len(self):\n",
    "        return self.seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RasaIntentEntityValidDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, markdown_lines: List[str], tokenizer, seq_len=128,):\n",
    "        self.intent_dict = {}\n",
    "        self.entity_dict = {}\n",
    "        self.entity_dict[\"O\"] = 0  # using BIO tagging\n",
    "\n",
    "        self.dataset = []\n",
    "        self.seq_len = seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        intent_value_list = []\n",
    "        entity_type_list = []\n",
    "\n",
    "        current_intent_focus = \"\"\n",
    "\n",
    "        text_list = []\n",
    "\n",
    "        for line in tqdm(markdown_lines, desc=\"Organizing Intent & Entity dictionary in NLU markdown file ...\",):\n",
    "            if len(line.strip()) < 2:\n",
    "                current_intent_focus = \"\"\n",
    "                continue\n",
    "\n",
    "            if \"## \" in line:\n",
    "                if \"intent:\" in line:\n",
    "                    intent_value_list.append(line.split(\":\")[1].strip())\n",
    "                    current_intent_focus = line.split(\":\")[1].strip()\n",
    "                else:\n",
    "                    current_intent_focus = \"\"\n",
    "\n",
    "            else:\n",
    "                if current_intent_focus != \"\":\n",
    "                    text = line[2:].strip()\n",
    "\n",
    "                    for type_str in re.finditer(r\"\\([a-zA-Z_1-2]+\\)\", text):\n",
    "                        entity_type = (text[type_str.start() + 1 : type_str.end() - 1].replace(\"(\", \"\").replace(\")\", \"\"))\n",
    "                        entity_type_list.append(entity_type)\n",
    "\n",
    "                    text = re.sub(r\"\\([a-zA-Z_1-2]+\\)\", \"\", text)  # remove (...) str\n",
    "                    text = text.replace(\"[\", \"\").replace(\"]\", \"\")  # remove '[',']' special char\n",
    "\n",
    "                    if len(text) > 0:\n",
    "                        text_list.append(text.strip())\n",
    "        \n",
    "        #dataset tokenizer setting\n",
    "        if \"ElectraTokenizer\" in str(type(tokenizer)):\n",
    "            self.tokenizer = tokenizer\n",
    "            self.pad_token_id = 0\n",
    "            self.unk_token_id = 1\n",
    "            self.eos_token_id = 3 #[SEP] token\n",
    "            self.bos_token_id = 2 #[CLS] token\n",
    "\n",
    "        else:\n",
    "            raise ValueError('not supported tokenizer type')\n",
    "\n",
    "\n",
    "\n",
    "        intent_value_list = sorted(intent_value_list)\n",
    "        for intent_value in intent_value_list:\n",
    "            if intent_value not in self.intent_dict.keys():\n",
    "                self.intent_dict[intent_value] = len(self.intent_dict)\n",
    "\n",
    "        entity_type_list = sorted(entity_type_list)\n",
    "        for entity_type in entity_type_list:\n",
    "            if entity_type + '_B' not in self.entity_dict.keys():\n",
    "                self.entity_dict[str(entity_type) + '_B'] = len(self.entity_dict)\n",
    "            if entity_type + '_I' not in self.entity_dict.keys():\n",
    "                self.entity_dict[str(entity_type) + '_I'] = len(self.entity_dict)\n",
    "\n",
    "        current_intent_focus = \"\"\n",
    "\n",
    "        for line in tqdm(markdown_lines, desc=\"Extracting Intent & Entity in NLU markdown files...\",):\n",
    "            if len(line.strip()) < 2:\n",
    "                current_intent_focus = \"\"\n",
    "                continue\n",
    "\n",
    "            if \"## \" in line:\n",
    "                if \"intent:\" in line:\n",
    "                    current_intent_focus = line.split(\":\")[1].strip()\n",
    "                else:\n",
    "                    current_intent_focus = \"\"\n",
    "            else:\n",
    "                if current_intent_focus != \"\":  # intent & entity sentence occur case\n",
    "                    text = line[2:]\n",
    "\n",
    "                    entity_value_list = []\n",
    "                    for value in re.finditer(r\"\\[(.*?)\\]\", text):\n",
    "                        entity_value_list.append(text[value.start() + 1 : value.end() - 1].replace(\"[\", \"\").replace(\"]\", \"\"))\n",
    "\n",
    "                    entity_type_list = []\n",
    "                    for type_str in re.finditer(r\"\\([a-zA-Z_1-2]+\\)\", text):\n",
    "                        entity_type = (text[type_str.start() + 1 : type_str.end() - 1].replace(\"(\", \"\").replace(\")\", \"\"))\n",
    "                        entity_type_list.append(entity_type)\n",
    "\n",
    "                    text = re.sub(r\"\\([a-zA-Z_1-2]+\\)\", \"\", text)  # remove (...) str\n",
    "                    text = text.replace(\"[\", \"\").replace(\"]\", \"\")  # remove '[',']' special char\n",
    "\n",
    "                    if len(text) > 0:\n",
    "                        each_data_dict = {}\n",
    "                        each_data_dict[\"text\"] = text.strip()\n",
    "                        each_data_dict[\"intent\"] = current_intent_focus\n",
    "                        each_data_dict[\"intent_idx\"] = self.intent_dict[current_intent_focus]\n",
    "                        each_data_dict[\"entities\"] = []\n",
    "\n",
    "                        for value, type_str in zip(entity_value_list, entity_type_list):\n",
    "                            for entity in re.finditer(value, text):\n",
    "                                entity_tokens = self.tokenize(value)\n",
    "\n",
    "                                for i, entity_token in enumerate(entity_tokens):\n",
    "                                    if i == 0:\n",
    "                                        BIO_type_str = type_str + '_B'\n",
    "                                    else:\n",
    "                                        BIO_type_str = type_str + '_I'\n",
    "\n",
    "                                    each_data_dict[\"entities\"].append(\n",
    "                                        {\n",
    "                                            \"start\": text.find(entity_token, entity.start(), entity.end()),\n",
    "                                            \"end\": text.find(entity_token, entity.start(), entity.end()) + len(entity_token),\n",
    "                                            \"entity\": type_str,\n",
    "                                            \"value\": entity_token,\n",
    "                                            \"entity_idx\": self.entity_dict[BIO_type_str],\n",
    "                                        }\n",
    "                                    )\n",
    "\n",
    "\n",
    "                        self.dataset.append(each_data_dict)\n",
    "\n",
    "        \n",
    "        print(f\"Intents: {self.intent_dict}\")\n",
    "        print(f\"Entities: {self.entity_dict}\")\n",
    "\n",
    "    def tokenize(self, text: str, skip_special_char=True):\n",
    "        if \"ElectraTokenizer\" in str(type(self.tokenizer)):\n",
    "            if skip_special_char:\n",
    "                return self.tokenizer.tokenize(text)\n",
    "            else:\n",
    "                return [token.replace('#','') for token in self.tokenizer.tokenize(text)]\n",
    "        else:\n",
    "            raise ValueError('not supported tokenizer type')\n",
    "            \n",
    "    def encode(self, text: str, padding: bool = True, return_tensor: bool = True):\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        if type(tokens) == list:\n",
    "            tokens = torch.tensor(tokens).long()\n",
    "        else:\n",
    "            tokens = tokens.long()\n",
    "\n",
    "        if padding:\n",
    "            if len(tokens) >= self.seq_len:\n",
    "                tokens = tokens[: self.seq_len]\n",
    "            else:\n",
    "                pad_tensor = torch.tensor([self.pad_token_id] * (self.seq_len - len(tokens)))\n",
    "            \n",
    "                tokens = torch.cat((tokens, pad_tensor), 0)\n",
    "\n",
    "        if return_tensor:\n",
    "            return tokens\n",
    "        else:\n",
    "            return tokens.numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.encode(self.dataset[idx][\"text\"])\n",
    "\n",
    "        intent_idx = torch.tensor([self.dataset[idx][\"intent_idx\"]])\n",
    "\n",
    "        entity_idx = np.array(self.seq_len * [0]) # O tag indicate 0(zero)\n",
    "\n",
    "        for entity_info in self.dataset[idx][\"entities\"]:\n",
    "            if \"ElectraTokenizer\" in str(type(self.tokenizer)):\n",
    "                ##check whether entity value is include in splitted token\n",
    "                for token_seq, token_value in enumerate(tokens):\n",
    "                    # Consider [CLS](bos) token\n",
    "                    if token_seq == 0:\n",
    "                        continue\n",
    "\n",
    "                    for entity_seq, entity_info in enumerate(self.dataset[idx][\"entities\"]):\n",
    "                        if (self.tokenizer.convert_ids_to_tokens([token_value.item()])[0] in entity_info[\"value\"]):\n",
    "                            entity_idx[token_seq] = entity_info[\"entity_idx\"]\n",
    "                            break\n",
    "\n",
    "        entity_idx = torch.from_numpy(entity_idx)\n",
    "        \n",
    "\n",
    "        return tokens, intent_idx, entity_idx, self.dataset[idx][\"text\"]\n",
    "\n",
    "    def get_intent_idx(self):\n",
    "        return self.intent_dict\n",
    "\n",
    "    def get_entity_idx(self):\n",
    "        return self.entity_dict\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.tokenizer.vocab_size\n",
    "\n",
    "    def get_seq_len(self):\n",
    "        return self.seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n",
    "#nlu_data_train = open(\"RASA_NLU_training_dataset.md\", encoding=\"utf-8\").readlines()\n",
    "#dataset_train = RasaIntentEntityValidDataset(markdown_lines=nlu_data_train, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(pred: list, label:list, label_index:dict, file_name:str=None, output_dir='results'):\n",
    "    cm = ConfusionMatrix(pred, label)\n",
    "    cm.relabel(mapping=label_index)\n",
    "    cm_matrix = cm.matrix\n",
    "    cm_normalized_matrix = cm.normalized_matrix\n",
    "\n",
    "    if file_name is None:\n",
    "        file_name = 'confusion_matrix.json'\n",
    "    \n",
    "    normalized_file_name = file_name.replace('.', '_normalized.')\n",
    "\n",
    "    if output_dir is not None:\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        with open(os.path.join(output_dir, file_name), 'w') as fp:\n",
    "            json.dump(cm_matrix, fp, indent=4)\n",
    "        \n",
    "        # with open(os.path.join(output_dir, normalized_file_name), 'w') as fp:\n",
    "        #     json.dump(cm_normalized_matrix, fp, indent=4)\n",
    "    \n",
    "    return cm_matrix\n",
    "\n",
    "\n",
    "def show_rasa_metrics(pred, label, labels=None, target_names=None, output_dir='results', file_name=None):\n",
    "\n",
    "    output = metrics.classification_report(label, pred, labels=labels, target_names=target_names, output_dict=True)\n",
    "                                           \n",
    "    if file_name is None:\n",
    "        file_name = 'reports.json'\n",
    "        \n",
    "    if output_dir is not None:\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        with open(os.path.join(output_dir, file_name), 'w') as fp:\n",
    "            json.dump(output, fp, indent=4)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def show_entity_metrics(pred, label,  output_dir='results', file_name=None):\n",
    "    entity_metric = Entity_Matrics(label, pred)\n",
    "\n",
    "    output = entity_metric.generate_report()\n",
    "                                           \n",
    "    if file_name is None:\n",
    "        file_name = 'reports.json'\n",
    "        \n",
    "    if output_dir is not None:\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        with open(os.path.join(output_dir, file_name), 'w') as fp:\n",
    "            json.dump(output, fp, indent=4)\n",
    "\n",
    "    return output\n",
    "\n",
    "class Entity_Matrics:\n",
    "    def __init__(self, sents_true_labels: Sequence[Sequence[Dict]], sents_pred_labels:Sequence[Sequence[Dict]]):\n",
    "        self.sents_true_labels = sents_true_labels\n",
    "        self.sents_pred_labels = sents_pred_labels \n",
    "        self.types = set(entity['entity'] for sent in sents_true_labels for entity in sent)\n",
    "        self.confusion_matrices = {type: {'TP': 0, 'TN': 0, 'FP': 0, 'FN': 0} for type in self.types}\n",
    "        self.scores = {type: {'p': 0, 'r': 0, 'f1': 0} for type in self.types}\n",
    "\n",
    "    def cal_confusion_matrices(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Calculate confusion matrices for all sentences.\"\"\"\n",
    "        for true_labels, pred_labels in zip(self.sents_true_labels, self.sents_pred_labels):\n",
    "            for true_label in true_labels: \n",
    "                entity_type = true_label['entity']\n",
    "                prediction_hit_count = 0 \n",
    "                for pred_label in pred_labels:\n",
    "                    if pred_label['entity'] != entity_type:\n",
    "                        continue\n",
    "                    if pred_label['start'] == true_label['start'] and pred_label['end'] == true_label['end'] and pred_label['value'] == true_label['value']: # TP\n",
    "                        self.confusion_matrices[entity_type]['TP'] += 1\n",
    "                        prediction_hit_count += 1\n",
    "                    elif ((pred_label['start'] == true_label['start']) or (pred_label['end'] == true_label['end'])) and pred_label['value'] != true_label['value']: # boundry error, count FN, FP\n",
    "                        self.confusion_matrices[entity_type]['FP'] += 1\n",
    "                        self.confusion_matrices[entity_type]['FN'] += 1\n",
    "                        prediction_hit_count += 1\n",
    "                if prediction_hit_count != 1: # FN, model cannot make a prediction for true_label\n",
    "                    self.confusion_matrices[entity_type]['FN'] += 1\n",
    "                prediction_hit_count = 0 # reset to default\n",
    "\n",
    "    def cal_scores(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Calculate precision, recall, f1.\"\"\"\n",
    "        confusion_matrices = self.confusion_matrices \n",
    "        scores = {type: {'p': 0, 'r': 0, 'f1': 0} for type in self.types}\n",
    "        \n",
    "        for entity_type, confusion_matrix in confusion_matrices.items():\n",
    "            if confusion_matrix['TP'] == 0 and confusion_matrix['FP'] == 0:\n",
    "                scores[entity_type]['p'] = 0\n",
    "            else:\n",
    "                scores[entity_type]['p'] = confusion_matrix['TP'] / (confusion_matrix['TP'] + confusion_matrix['FP'])\n",
    "\n",
    "            if confusion_matrix['TP'] == 0 and confusion_matrix['FN'] == 0:\n",
    "                scores[entity_type]['r'] = 0\n",
    "            else:\n",
    "                scores[entity_type]['r'] = confusion_matrix['TP'] / (confusion_matrix['TP'] + confusion_matrix['FN']) \n",
    "\n",
    "            if scores[entity_type]['p'] == 0 or scores[entity_type]['r'] == 0:\n",
    "                scores[entity_type]['f1'] = 0\n",
    "            else:\n",
    "                scores[entity_type]['f1'] = 2*scores[entity_type]['p']*scores[entity_type]['r'] / (scores[entity_type]['p']+scores[entity_type]['r'])  \n",
    "        self.scores = scores\n",
    "\n",
    "    def print_confusion_matrices(self):\n",
    "        for entity_type, matrix in self.confusion_matrices.items():\n",
    "            print(f\"{entity_type}: {matrix}\")\n",
    "\n",
    "    def print_scores(self):\n",
    "        for entity_type, score in self.scores.items():\n",
    "            print(f\"{entity_type}: f1 {score['f1']:.4f}, precision {score['p']:.4f}, recall {score['r']:.4f}\")\n",
    "        \n",
    "    def cal_micro_avg(self):\n",
    "        sum_TP = 0\n",
    "        sum_FP = 0\n",
    "        sum_FN = 0\n",
    "        support = 0\n",
    "        for k, v in self.confusion_matrices.items():\n",
    "            sum_TP += v['TP']\n",
    "            sum_FP += v['FP']\n",
    "            sum_FN += v['FN']\n",
    "            support += np.array(list(self.confusion_matrices[k].values())).sum().item()\n",
    "        precision = sum_TP / (sum_TP + sum_FP)\n",
    "        recall = sum_TP / (sum_TP + sum_FN)\n",
    "        f1 = 2*(precision * recall / (precision + recall))\n",
    "        self.micro_avg = dict()\n",
    "        self.micro_avg['precision'] = precision\n",
    "        self.micro_avg['recall'] = recall\n",
    "        self.micro_avg['f1-score'] = f1\n",
    "        self.micro_avg['support'] = support\n",
    "    \n",
    "    def cal_macro_avg(self):\n",
    "        precision = []\n",
    "        recall = []\n",
    "        support = 0\n",
    "        for k, v in self.scores.items():\n",
    "            precision.append(v['p'])\n",
    "            recall.append(v['r'])\n",
    "        for k, v in self.confusion_matrices.items():\n",
    "            support += np.array(list(self.confusion_matrices[k].values())).sum().item()\n",
    "        precision = np.array(precision).mean()\n",
    "        recall = np.array(recall).mean()\n",
    "        f1 = 2*(precision * recall / (precision + recall))\n",
    "        self.macro_avg = dict()\n",
    "        self.macro_avg['precision'] = precision\n",
    "        self.macro_avg['recall'] = recall\n",
    "        self.macro_avg['f1-score'] = f1\n",
    "        self.macro_avg['support'] = support\n",
    "    \n",
    "    def cal_weight_avg(self):\n",
    "        tp = []\n",
    "        fp = []\n",
    "        fn = []\n",
    "        weight = []\n",
    "        support = 0\n",
    "        for k, v in self.confusion_matrices.items():\n",
    "            tp.append(v['TP'])\n",
    "            fp.append(v['FP'])\n",
    "            fn.append(v['FN'])\n",
    "            weight.append(np.array(list(v.values())).sum().item())\n",
    "            support += np.array(list(self.confusion_matrices[k].values())).sum().item()\n",
    "\n",
    "        weight = np.array(weight) / np.array(weight).sum()\n",
    "        tp = np.array(tp)\n",
    "        fp = np.array(fp)\n",
    "        fn = np.array(fn)\n",
    "        precision = (weight * tp).sum() / ((weight * tp).sum() + (weight * fp).sum())\n",
    "        recall = (weight * tp).sum() / ((weight * tp).sum() + (weight * fn).sum())\n",
    "        f1 = 2*(precision * recall / (precision + recall))\n",
    "        self.weight_avg = dict()\n",
    "        self.weight_avg['precision'] = precision.item()\n",
    "        self.weight_avg['recall'] = recall.item()\n",
    "        self.weight_avg['f1-score'] = f1.item()\n",
    "        self.weight_avg['support'] = support\n",
    "    \n",
    "    def generate_report(self):\n",
    "        self.cal_confusion_matrices()\n",
    "        self.cal_scores()\n",
    "        self.cal_micro_avg()\n",
    "        self.cal_macro_avg()\n",
    "        self.cal_weight_avg()\n",
    "        \n",
    "        report = dict()\n",
    "        for k, v in self.scores.items():\n",
    "            report[k] = dict()\n",
    "            report[k]['precision'] = v['p']\n",
    "            report[k]['recall'] = v['r']\n",
    "            report[k]['f1-score'] = v['f1']\n",
    "            report[k]['support'] = np.array(list(self.confusion_matrices[k].values())).sum().item()\n",
    "        report['micro avg'] = self.micro_avg\n",
    "        report['macro avg'] = self.macro_avg\n",
    "        report['weighted avg'] = self.weight_avg\n",
    "        return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDecoder(object):\n",
    "\n",
    "    def __init__(self, entity_dict:dict, tokenizer):\n",
    "        self.entity_dict = entity_dict\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def is_same_entity(self, i, j):\n",
    "        # check whether XXX_B, XXX_I tag are same \n",
    "        return self.entity_dict[i][:self.entity_dict[i].rfind('_')].strip() == self.entity_dict[j][:self.entity_dict[j].rfind('_')].strip()\n",
    "\n",
    "    def process(self, tokens, entity_indices, text):\n",
    "        # mapping entity result\n",
    "        entities = []\n",
    "        start_idx = -1\n",
    "\n",
    "        entity_indices = entity_indices.tolist()[:len(text)]\n",
    "        start_token_position = -1\n",
    "\n",
    "        # except first sequnce token whcih indicate BOS or [CLS] token\n",
    "        if type(tokens) == torch.Tensor:\n",
    "            tokens = tokens.long().tolist()\n",
    "\n",
    "        for i, entity_idx_value in enumerate(entity_indices):\n",
    "            if entity_idx_value != 0 and start_token_position == -1:\n",
    "                start_token_position = i\n",
    "            elif start_token_position >= 0 and not self.is_same_entity(entity_indices[i-1],entity_indices[i]):\n",
    "                end_token_position = i - 1\n",
    "\n",
    "                #print ('start_token_position')\n",
    "                #print (start_token_position)\n",
    "                #print ('end_token_position')\n",
    "                #print (end_token_position)\n",
    "\n",
    "                # find start text position\n",
    "                token_idx = tokens[start_token_position + 1]\n",
    "                if \"ElectraTokenizer\" in str(\n",
    "                    type(self.tokenizer)\n",
    "                ):  # ElectraTokenizer\n",
    "                    token_value = self.tokenizer.convert_ids_to_tokens([token_idx])[0].replace(\"#\", \"\")\n",
    "\n",
    "                if len(token_value.strip()) == 0:\n",
    "                    start_token_position = -1\n",
    "                    continue\n",
    "                    \n",
    "                start_position = text.find(token_value.strip())\n",
    "\n",
    "                # find end text position\n",
    "                token_idx = tokens[end_token_position + 1]\n",
    "                if \"ElectraTokenizer\" in str(\n",
    "                    type(self.tokenizer)\n",
    "                ):  # ElectraTokenizer\n",
    "                    token_value = self.tokenizer.convert_ids_to_tokens([token_idx])[0].replace(\"#\", \"\")\n",
    "\n",
    "                end_position = text.find(token_value.strip(), start_position) + len(token_value.strip())\n",
    "\n",
    "                if self.entity_dict[entity_indices[i-1]] != \"O\": # ignore 'O' tag\n",
    "                    entities.append(\n",
    "                        {\n",
    "                            \"start\": start_position,\n",
    "                            \"end\": end_position,\n",
    "                            \"value\": text[start_position:end_position],\n",
    "                            \"entity\": self.entity_dict[entity_indices[i-1]][:self.entity_dict[entity_indices[i-1]].rfind('_')]\n",
    "                        }\n",
    "                    )\n",
    "                        \n",
    "                    start_token_position = -1\n",
    "\n",
    "                if entity_idx_value == 0:\n",
    "                    start_token_position = -1\n",
    "\n",
    "\n",
    "        return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_intent_report(dataset, pl_module, tokenizer, file_name=None, output_dir=None, cuda=True):\n",
    "    ##generate rasa performance matrics\n",
    "    # text = []\n",
    "    preds = np.array([])\n",
    "    targets = np.array([])\n",
    "    logits = np.array([])\n",
    "    label_dict = dict()\n",
    "    pl_module.model.eval()\n",
    "    for k, v in pl_module.intent_dict.items():\n",
    "        label_dict[int(k)] = v\n",
    "    dataloader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"load intent dataset\"):\n",
    "        #dataset follows RasaIntentEntityDataset which defined in this package\n",
    "        input_ids, intent_idx, entity_idx, text = batch\n",
    "        model =  pl_module.model\n",
    "        if cuda > 0:\n",
    "            input_ids = input_ids.cuda()\n",
    "            model = model.cuda()\n",
    "        intent_pred, entity_pred = model.forward(input_ids)\n",
    "        y_label = intent_pred.argmax(1).cpu().numpy()\n",
    "        preds = np.append(preds, y_label)\n",
    "        targets = np.append(targets, intent_idx.cpu().numpy())\n",
    "        \n",
    "        logit = intent_pred.detach().cpu()\n",
    "        softmax = torch.nn.Softmax(dim=-1)\n",
    "        logit = softmax(logit).numpy()\n",
    "        logits = np.append(logits, logit.max(-1))\n",
    "    \n",
    "    preds = preds.astype(int)\n",
    "    targets = targets.astype(int)\n",
    "\n",
    "    labels = list(label_dict.keys())\n",
    "    target_names = list(label_dict.values())\n",
    "    \n",
    "    report = show_rasa_metrics(pred=preds, label=targets, labels=labels, target_names=target_names, file_name=file_name, output_dir=output_dir)\n",
    "\n",
    "def show_entity_report(dataset, pl_module, tokenizer, file_name=None, output_dir=None, cuda=True):\n",
    "    \n",
    "    ##generate rasa performance matrics\n",
    "    text = []\n",
    "    label_dict = dict()\n",
    "    pl_module.model.eval()\n",
    "    for k, v in pl_module.entity_dict.items():\n",
    "        label_dict[int(k)] = v\n",
    "\n",
    "    decoder = NERDecoder(label_dict, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "    preds = list()\n",
    "    targets = list()\n",
    "    labels = set()\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"load entity dataset\"):\n",
    "        input_ids, intent_idx, entity_idx, token = batch\n",
    "        text.extend(token)\n",
    "        if cuda > 0:\n",
    "            input_ids = input_ids.cuda()\n",
    "        _, entity_result = pl_module.model.forward(input_ids)\n",
    "\n",
    "        entity_result = entity_result.detach().cpu()\n",
    "        _, entity_indices = torch.max(entity_result, dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(entity_idx.shape[0]):\n",
    "            decode_original = decoder.process(input_ids[i].cpu().numpy(), entity_idx[i].numpy(), token[i])\n",
    "            decode_pred = decoder.process(input_ids[i].cpu().numpy(), entity_indices[i].numpy(), token[i])\n",
    "            targets.append(decode_original)\n",
    "            preds.append(decode_pred)\n",
    "\n",
    "\n",
    "    report = show_entity_metrics(pred=preds, label=targets, file_name=file_name, output_dir=output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerfCallback(Callback):\n",
    "    def __init__(self, file_path=None, gpu_num=0, report_nm=None, output_dir=None, root_path=None):\n",
    "        self.file_path = file_path\n",
    "        if gpu_num > 0:\n",
    "            self.cuda = True\n",
    "        else:\n",
    "            self.cuda = False\n",
    "        self.report_nm = report_nm\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        if root_path is None:\n",
    "            self.root_path = 'lightning_logs'\n",
    "        else:\n",
    "            self.root_path = os.path.join(root_path, 'lightning_logs')\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):        \n",
    "        if self.file_path is None:\n",
    "            print(\"evaluate valid data\")\n",
    "            dataset = pl_module.val_dataset\n",
    "            tokenizer = pl_module.dataset.tokenizer\n",
    "        else:\n",
    "            print(\"evaluate new data\")\n",
    "            tokenizer = pl_module.model.dataset.tokenizer\n",
    "            self.nlu_data = open(self.file_path, encoding=\"utf-8\").readlines()\n",
    "            dataset = RasaIntentEntityValidDataset(markdown_lines=self.nlu_data, tokenizer=tokenizer)\n",
    "                \n",
    "        if self.output_dir is None:\n",
    "            folder_path = [f for f in glob.glob(os.path.join(self.root_path, \"**/\"), recursive=False)]\n",
    "            folder_path.sort()\n",
    "            self.output_dir  = folder_path[-1]\n",
    "        self.output_dir = os.path.join(self.output_dir, 'results')\n",
    "        intent_report_nm = self.report_nm.replace('.', '_intent.')\n",
    "        entity_report_nm = self.report_nm.replace('.', '_entity.')\n",
    "        show_intent_report(dataset, pl_module, tokenizer, file_name=intent_report_nm, output_dir=self.output_dir, cuda=self.cuda)\n",
    "        show_entity_report(dataset, pl_module, tokenizer, file_name=entity_report_nm, output_dir=self.output_dir, cuda=self.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: None,\n",
    "        vocab_size: int,\n",
    "        seq_len: int,\n",
    "        intent_class_num: int,\n",
    "        entity_class_num: int,\n",
    "        d_model=512,\n",
    "        nhead=8,\n",
    "        num_encoder_layers=6,\n",
    "        dim_feedforward=2048,\n",
    "        dropout=0.1,\n",
    "        activation=\"relu\",\n",
    "        pad_token_id: int = 0,\n",
    "    ):\n",
    "        super(EmbeddingTransformer, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.seq_len = seq_len\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "        if backbone is None:\n",
    "            self.encoder = nn.TransformerEncoder(\n",
    "                TransformerEncoderLayer(\n",
    "                    d_model, nhead, dim_feedforward, dropout, activation\n",
    "                ),\n",
    "                num_encoder_layers,\n",
    "                LayerNorm(d_model),\n",
    "            )\n",
    "        else:  # pre-defined model architecture use\n",
    "            if backbone == \"electra\":\n",
    "                self.encoder = ElectraModel.from_pretrained(\"google/electra-small-discriminator\")\n",
    "\n",
    "            d_model = self.encoder.config.hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(self.seq_len, d_model)\n",
    "\n",
    "        self.intent_feature = nn.Linear(d_model, intent_class_num)\n",
    "        self.entity_feature = nn.Linear(d_model, entity_class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.backbone in [\"electra\"]:\n",
    "            feature = self.encoder(x)\n",
    "\n",
    "            if type(feature) == tuple:\n",
    "                feature = feature[0]  # last_hidden_state (N,S,E)\n",
    "\n",
    "            # first token in sequence used to intent classification\n",
    "            intent_feature = self.intent_feature(feature[:, 0, :]) # (N,E) -> (N,i_C)\n",
    "\n",
    "            # other tokens in sequence used to entity classification\n",
    "            entity_feature = self.entity_feature(feature[:, :, :]) # (N,S,E) -> (N,S,e_C)\n",
    "\n",
    "            return intent_feature, entity_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualIntentEntityTransformer(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hparams = hparams\n",
    "        if type(self.hparams) == dict:\n",
    "            self.hparams = Namespace(**self.hparams)\n",
    "\n",
    "        self.dataset = RasaIntentEntityDataset(\n",
    "            markdown_lines=self.hparams.nlu_data,\n",
    "            tokenizer=self.hparams.tokenizer,\n",
    "        )\n",
    "\n",
    "        self.model = EmbeddingTransformer(\n",
    "            backbone=self.hparams.backbone,\n",
    "            vocab_size=self.dataset.get_vocab_size(),\n",
    "            seq_len=self.dataset.get_seq_len(),\n",
    "            intent_class_num=len(self.dataset.get_intent_idx()),\n",
    "            entity_class_num=len(self.dataset.get_entity_idx()),\n",
    "            d_model=self.hparams.d_model,\n",
    "            num_encoder_layers=self.hparams.num_encoder_layers,\n",
    "            pad_token_id=self.dataset.pad_token_id\n",
    "        )\n",
    "\n",
    "        self.train_ratio = self.hparams.train_ratio\n",
    "        self.batch_size = self.hparams.batch_size\n",
    "        self.optimizer = self.hparams.optimizer\n",
    "        self.intent_optimizer_lr = self.hparams.intent_optimizer_lr\n",
    "        self.entity_optimizer_lr = self.hparams.entity_optimizer_lr\n",
    "\n",
    "        self.intent_loss_fn = nn.CrossEntropyLoss()\n",
    "        # reduce O tag class weight to figure out entity imbalance distribution\n",
    "        self.entity_loss_fn = nn.CrossEntropyLoss(weight=torch.Tensor([0.1] + [1.0] * (len(self.dataset.get_entity_idx()) - 1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        train_length = int(len(self.dataset) * self.train_ratio)\n",
    "\n",
    "        self.train_dataset, self.val_dataset = random_split(\n",
    "            self.dataset, [train_length, len(self.dataset) - train_length],\n",
    "        )\n",
    "\n",
    "        self.hparams.intent_label = self.get_intent_label()\n",
    "        self.hparams.entity_label = self.get_entity_label()\n",
    "    \n",
    "    def get_intent_label(self):\n",
    "        self.intent_dict = {}\n",
    "        for k, v in self.dataset.intent_dict.items():\n",
    "            self.intent_dict[str(v)] = k\n",
    "        return self.intent_dict \n",
    "    \n",
    "    def get_entity_label(self):\n",
    "        self.entity_dict = {}\n",
    "        for k, v in self.dataset.entity_dict.items():\n",
    "            self.entity_dict[str(v)] = k\n",
    "        return self.entity_dict\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "        return val_loader\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        intent_optimizer = eval(\n",
    "            f\"{self.optimizer}(self.parameters(), lr={self.intent_optimizer_lr})\"\n",
    "        )\n",
    "        entity_optimizer = eval(\n",
    "            f\"{self.optimizer}(self.parameters(), lr={self.entity_optimizer_lr})\"\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            [intent_optimizer, entity_optimizer],\n",
    "            # [StepLR(intent_optimizer, step_size=1),StepLR(entity_optimizer, step_size=1),],\n",
    "            [\n",
    "                ReduceLROnPlateau(intent_optimizer, patience=1),\n",
    "                ReduceLROnPlateau(entity_optimizer, patience=1),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        self.model.train()\n",
    "\n",
    "        tokens, intent_idx, entity_idx, text = batch\n",
    "        intent_pred, entity_pred = self.forward(tokens)\n",
    "\n",
    "        intent_acc = get_accuracy(intent_pred.argmax(1).cpu(), intent_idx.cpu())[0]\n",
    "        #entity_acc = get_token_accuracy(entity_pred.argmax(2), entity_idx, ignore_index=self.dataset.pad_token_id)[0]\n",
    "\n",
    "        tensorboard_logs = {\n",
    "            \"train/intent/acc\": intent_acc,\n",
    "            #\"train/entity/acc\": entity_acc,\n",
    "        }\n",
    "\n",
    "        if optimizer_idx == 0:\n",
    "            intent_loss = self.intent_loss_fn(intent_pred, intent_idx.squeeze(1))\n",
    "            tensorboard_logs[\"train/intent/loss\"] = intent_loss\n",
    "\n",
    "            return {\n",
    "                \"loss\": intent_loss,\n",
    "                \"log\": tensorboard_logs,\n",
    "            }\n",
    "\n",
    "        if optimizer_idx == 1:\n",
    "            entity_loss = self.entity_loss_fn(entity_pred.transpose(1, 2), entity_idx.long(),)\n",
    "            tensorboard_logs[\"train/entity/loss\"] = entity_loss\n",
    "\n",
    "            return {\n",
    "                \"loss\": entity_loss,\n",
    "                \"log\": tensorboard_logs,\n",
    "            }\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.model.eval()\n",
    "\n",
    "        tokens, intent_idx, entity_idx, text = batch\n",
    "        intent_pred, entity_pred = self.forward(tokens)\n",
    "        \n",
    "\n",
    "        intent_acc = get_accuracy(intent_pred.argmax(1).cpu(), intent_idx.cpu())[0]\n",
    "        #entity_acc = get_token_accuracy(entity_pred.argmax(2), entity_idx, ignore_index=self.dataset.pad_token_id)[0]\n",
    "        intent_f1 = f1_score(intent_pred.argmax(1), intent_idx)\n",
    "\n",
    "        intent_loss = self.intent_loss_fn(intent_pred, intent_idx.squeeze(1))\n",
    "        entity_loss = self.entity_loss_fn(entity_pred.transpose(1, 2), entity_idx.long(),)\n",
    "\n",
    "        return {\n",
    "            \"val_intent_acc\": torch.Tensor([intent_acc]),\n",
    "            #\"val_entity_acc\": torch.Tensor([entity_acc]),\n",
    "            \"val_intent_f1\": intent_f1,\n",
    "            \"val_intent_loss\": intent_loss,\n",
    "            \"val_entity_loss\": entity_loss,\n",
    "            \"val_loss\": intent_loss + entity_loss,\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        avg_intent_acc = torch.stack([x[\"val_intent_acc\"] for x in outputs]).mean()\n",
    "        #avg_entity_acc = torch.stack([x[\"val_entity_acc\"] for x in outputs]).mean()\n",
    "        avg_intent_f1 = torch.stack([x[\"val_intent_f1\"] for x in outputs]).mean()\n",
    "\n",
    "        tensorboard_logs = {\n",
    "            \"val/loss\": avg_loss,\n",
    "            \"val/intent_acc\": avg_intent_acc,\n",
    "            #\"val/entity_acc\": avg_entity_acc,\n",
    "            \"val/intent_f1\": avg_intent_f1,\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"val_loss\": avg_loss,\n",
    "            \"val_intent_f1\": avg_intent_f1,\n",
    "            #\"val_entity_acc\": avg_entity_acc,\n",
    "            \"log\": tensorboard_logs,\n",
    "            \"progress_bar\": tensorboard_logs,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    file_path,\n",
    "    # training args\n",
    "    train_ratio=0.8,\n",
    "    batch_size=None,\n",
    "    optimizer=\"Adam\",\n",
    "    intent_optimizer_lr=1e-5,\n",
    "    entity_optimizer_lr=2e-5,\n",
    "    checkpoint_path=os.getcwd(),\n",
    "    max_epochs=20,\n",
    "    tokenizer_type=\"char\",\n",
    "    # model args\n",
    "    # refered below link to optimize model\n",
    "    # https://www.notion.so/A-Primer-in-BERTology-What-we-know-about-how-BERT-works-aca45feaba2747f09f1a3cdd1b1bbe16\n",
    "    backbone=None,\n",
    "    d_model=256,\n",
    "    num_encoder_layers=2,\n",
    "    **kwargs\n",
    "):\n",
    "    gpu_num = torch.cuda.device_count()\n",
    "\n",
    "    if backbone is None:\n",
    "        report_nm = \"diet_{}_tokenizer_report.json\".format(tokenizer_type)\n",
    "    else:\n",
    "        report_nm = \"{}_report.json\".format(backbone)\n",
    "\n",
    "    if batch_size is not None:\n",
    "        trainer = Trainer(\n",
    "            default_root_dir=checkpoint_path,\n",
    "            max_epochs=max_epochs,\n",
    "            gpus=gpu_num,\n",
    "            callbacks=[\n",
    "                PerfCallback(\n",
    "                    gpu_num=gpu_num, report_nm=report_nm, root_path=checkpoint_path\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "    else:\n",
    "        trainer = Trainer(\n",
    "            default_root_dir=checkpoint_path,\n",
    "            max_epochs=max_epochs,\n",
    "            gpus=gpu_num,\n",
    "            auto_scale_batch_size='binsearch',\n",
    "            callbacks=[\n",
    "                PerfCallback(\n",
    "                    gpu_num=gpu_num, report_nm=report_nm, root_path=checkpoint_path\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    model_args = {}\n",
    "\n",
    "    # training args\n",
    "    model_args[\"max_epochs\"] = max_epochs\n",
    "    model_args[\"nlu_data\"] = open(file_path, encoding=\"utf-8\").readlines()\n",
    "    model_args[\"train_ratio\"] = train_ratio\n",
    "    model_args[\"batch_size\"] = batch_size\n",
    "    model_args[\"optimizer\"] = optimizer\n",
    "    model_args[\"intent_optimizer_lr\"] = intent_optimizer_lr\n",
    "    model_args[\"entity_optimizer_lr\"] = entity_optimizer_lr\n",
    "\n",
    "    if backbone is None:\n",
    "        model_args[\"tokenizer\"] = tokenizer_type\n",
    "\n",
    "    else:\n",
    "        if backbone == \"electra\":\n",
    "            model_args[\"tokenizer\"] = ElectraTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n",
    "\n",
    "    # model args\n",
    "    model_args[\"backbone\"] = backbone\n",
    "    model_args[\"d_model\"] = d_model\n",
    "    model_args[\"num_encoder_layers\"] = num_encoder_layers\n",
    "\n",
    "    for key, value in kwargs.items():\n",
    "        model_args[key] = value\n",
    "\n",
    "    hparams = Namespace(**model_args)\n",
    "\n",
    "    model = DualIntentEntityTransformer(hparams)\n",
    "\n",
    "    trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "Organizing Intent & Entity dictionary in NLU markdown file ...: 100%|██████████| 991/991 [00:00<00:00, 104046.54it/s]\n",
      "Extracting Intent & Entity in NLU markdown files...: 100%|██████████| 991/991 [00:00<00:00, 32986.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intents: {'affirmative': 0, 'anythingElse': 1, 'bye': 2, 'greet': 3, 'listProjects': 4, 'listTasks': 5, 'negative': 6, 'nothingElse': 7}\n",
      "Entities: {'O': 0, 'priority_B': 1, 'priority_I': 2, 'state_B': 3, 'state_I': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name           | Type                 | Params\n",
      "--------------------------------------------------------\n",
      "0 | model          | EmbeddingTransformer | 21 M  \n",
      "1 | intent_loss_fn | CrossEntropyLoss     | 0     \n",
      "2 | entity_loss_fn | CrossEntropyLoss     | 0     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f84b15527f248ad8e13d7224b2e7c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n",
      "\n",
      "load intent dataset:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "load intent dataset:  14%|█▍        | 1/7 [00:00<00:00,  7.71it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate valid data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "load intent dataset:  29%|██▊       | 2/7 [00:00<00:00,  7.70it/s]\u001b[A\n",
      "load intent dataset:  43%|████▎     | 3/7 [00:00<00:00,  7.88it/s]\u001b[A\n",
      "load intent dataset:  57%|█████▋    | 4/7 [00:00<00:00,  7.80it/s]\u001b[A\n",
      "load intent dataset:  71%|███████▏  | 5/7 [00:00<00:00,  7.79it/s]\u001b[A\n",
      "load intent dataset: 100%|██████████| 7/7 [00:00<00:00,  8.64it/s]\u001b[A\n",
      "\n",
      "load entity dataset:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "load entity dataset:  14%|█▍        | 1/7 [00:00<00:00,  7.69it/s]\u001b[A\n",
      "load entity dataset:  29%|██▊       | 2/7 [00:00<00:00,  7.69it/s]\u001b[A\n",
      "load entity dataset:  43%|████▎     | 3/7 [00:00<00:00,  7.61it/s]\u001b[A\n",
      "load entity dataset:  57%|█████▋    | 4/7 [00:00<00:00,  7.38it/s]\u001b[A\n",
      "load entity dataset:  71%|███████▏  | 5/7 [00:00<00:00,  7.37it/s]\u001b[A\n",
      "load entity dataset: 100%|██████████| 7/7 [00:00<00:00,  8.17it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    " train(\"Intent_Entity_dataset.md\",\n",
    "       #training args\n",
    "        train_ratio=0.8,\n",
    "        batch_size=32,\n",
    "        optimizer=\"Adam\",\n",
    "        intent_optimizer_lr=1e-5,\n",
    "        entity_optimizer_lr=2e-5,\n",
    "        checkpoint_path=os.getcwd(),\n",
    "        max_epochs=20,\n",
    "        backbone=\"electra\",\n",
    "\n",
    "        #model args\n",
    "        num_encoder_layers=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "intent_dict = {}\n",
    "entity_dict = {}\n",
    "\n",
    "class Inferencer:\n",
    "    def __init__(self, checkpoint_path: str):\n",
    "        self.model = DualIntentEntityTransformer.load_from_checkpoint(checkpoint_path)\n",
    "        self.model.model.eval()\n",
    "\n",
    "        self.intent_dict = {}\n",
    "        for k, v in self.model.dataset.intent_dict.items():\n",
    "            self.intent_dict[v] = k # str key -> int key\n",
    "\n",
    "        self.entity_dict = {}\n",
    "        for k, v in self.model.dataset.entity_dict.items():\n",
    "            self.entity_dict[v] = k # str key -> int key\n",
    "\n",
    "        logging.info(\"intent dictionary\")\n",
    "        logging.info(self.intent_dict)\n",
    "        print()\n",
    "\n",
    "        logging.info(\"entity dictionary\")\n",
    "        logging.info(self.entity_dict)\n",
    "\n",
    "    def is_same_entity(self, i, j):\n",
    "        # check whether XXX_B, XXX_I tag are same \n",
    "        return self.entity_dict[i][:self.entity_dict[i].rfind('_')].strip() == self.entity_dict[j][:self.entity_dict[j].rfind('_')].strip()\n",
    "\n",
    "    def inference(self, text: str, intent_topk=5):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\n",
    "                \"model is not loaded, first call load_model(checkpoint_path)\"\n",
    "            )\n",
    "\n",
    "        text = text.strip().lower()\n",
    "\n",
    "        # encode text to token_indices\n",
    "        tokens = self.model.dataset.encode(text)\n",
    "        intent_result, entity_result = self.model.forward(tokens.unsqueeze(0).cpu())\n",
    "\n",
    "        # mapping intent result\n",
    "        rank_values, rank_indicies = torch.topk(\n",
    "            nn.Softmax(dim=1)(intent_result)[0], k=intent_topk\n",
    "        )\n",
    "        intent = {}\n",
    "        intent_ranking = []\n",
    "        for i, (value, index) in enumerate(\n",
    "            list(zip(rank_values.tolist(), rank_indicies.tolist()))\n",
    "        ):\n",
    "            intent_ranking.append(\n",
    "                {\"confidence\": value, \"name\": self.intent_dict[index]}\n",
    "            )\n",
    "\n",
    "            if i == 0:\n",
    "                intent[\"name\"] = self.intent_dict[index]\n",
    "                intent[\"confidence\"] = value\n",
    "\n",
    "        # mapping entity result\n",
    "        entities = []\n",
    "\n",
    "        # except first & last sequnce token whcih indicate BOS or [CLS] token & EOS or [SEP] token\n",
    "        _, entity_indices = torch.max((entity_result)[0][1:-1, :], dim=1)\n",
    "        start_idx = -1\n",
    "\n",
    "        #print ('tokens')\n",
    "        #print (tokens)\n",
    "        #print ('predicted entities')\n",
    "        #print (entity_indices)\n",
    "\n",
    "        entity_indices = entity_indices.tolist()[:len(text)]\n",
    "        start_token_position = -1\n",
    "\n",
    "        # except first sequnce token whcih indicate BOS or [CLS] token\n",
    "        if type(tokens) == torch.Tensor:\n",
    "            tokens = tokens.long().tolist()\n",
    "\n",
    "        for i, entity_idx_value in enumerate(entity_indices):\n",
    "            if entity_idx_value != 0 and start_token_position == -1:\n",
    "                start_token_position = i\n",
    "            elif start_token_position >= 0 and not self.is_same_entity(entity_indices[i-1],entity_indices[i]):\n",
    "                end_token_position = i - 1\n",
    "\n",
    "                #print ('start_token_position')\n",
    "                #print (start_token_position)\n",
    "                #print ('end_token_position')\n",
    "                #print (end_token_position)\n",
    "\n",
    "                # find start text position\n",
    "                token_idx = tokens[start_token_position + 1]\n",
    "                if \"ElectraTokenizer\" in str(\n",
    "                    type(self.model.dataset.tokenizer)\n",
    "                ):  # ElectraTokenizer\n",
    "                    token_value = self.model.dataset.tokenizer.convert_ids_to_tokens([token_idx])[0].replace(\"#\", \"\")\n",
    "\n",
    "                if len(token_value.strip()) == 0:\n",
    "                    start_token_position = -1\n",
    "                    continue\n",
    "                  \n",
    "                start_position = text.find(token_value.strip())\n",
    "\n",
    "                # find end text position\n",
    "                token_idx = tokens[end_token_position + 1]\n",
    "                if \"ElectraTokenizer\" in str(\n",
    "                    type(self.model.dataset.tokenizer)\n",
    "                ):  # ElectraTokenizer\n",
    "                    token_value = self.model.dataset.tokenizer.convert_ids_to_tokens([token_idx])[0].replace(\"#\", \"\")\n",
    "\n",
    "                end_position = text.find(token_value.strip(), start_position) + len(token_value.strip())\n",
    "\n",
    "                if self.entity_dict[entity_indices[i-1]] != \"O\": # ignore 'O' tag\n",
    "                    entities.append(\n",
    "                         {\n",
    "                            \"start\": start_position,\n",
    "                            \"end\": end_position,\n",
    "                            \"value\": text[start_position:end_position],\n",
    "                            \"entity\": self.entity_dict[entity_indices[i-1]][:self.entity_dict[entity_indices[i-1]].rfind('_')]\n",
    "                        }\n",
    "                    )\n",
    "                      \n",
    "                    start_token_position = -1\n",
    "\n",
    "                if entity_idx_value == 0:\n",
    "                    start_token_position = -1\n",
    "\n",
    "        result = {\n",
    "            \"text\": text,\n",
    "            \"intent\": intent,\n",
    "            \"intent_ranking\": intent_ranking,\n",
    "            \"entities\": entities,\n",
    "        }\n",
    "\n",
    "        # print (result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = Inferencer(\"lightning_logs/version_35/checkpoints/epoch=19.ckpt\")\n",
    "inferencer.inference(\"list of tasks\", intent_topk=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
