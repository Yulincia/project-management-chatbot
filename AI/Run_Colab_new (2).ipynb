{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 845
    },
    "executionInfo": {
     "elapsed": 12905,
     "status": "ok",
     "timestamp": 1601569224058,
     "user": {
      "displayName": "Samah Anemiche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji0DTjo2tp998fKQnRGzcFOAI6CDc0uLFY7rYU=s64",
      "userId": "17890150816051632495"
     },
     "user_tz": -120
    },
    "id": "nr800kFxIk6x",
    "outputId": "6ce95d0c-2bc5-4130-9ae3-6bba29a26724"
   },
   "outputs": [],
   "source": [
    "! pip install transformers==2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 21967,
     "status": "ok",
     "timestamp": 1601569233410,
     "user": {
      "displayName": "Samah Anemiche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji0DTjo2tp998fKQnRGzcFOAI6CDc0uLFY7rYU=s64",
      "userId": "17890150816051632495"
     },
     "user_tz": -120
    },
    "id": "JiW7wsP1Ik67",
    "outputId": "d4bf9294-ae2c-4380-9047-54c1a9df4db9"
   },
   "outputs": [],
   "source": [
    "! pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "executionInfo": {
     "elapsed": 25476,
     "status": "ok",
     "timestamp": 1601569237123,
     "user": {
      "displayName": "Samah Anemiche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji0DTjo2tp998fKQnRGzcFOAI6CDc0uLFY7rYU=s64",
      "userId": "17890150816051632495"
     },
     "user_tz": -120
    },
    "id": "HCigcYVhIk7F",
    "outputId": "a6740d5a-4ad9-4af6-f701-83ad0b850cd8"
   },
   "outputs": [],
   "source": [
    "! pip install pytorch-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "executionInfo": {
     "elapsed": 29739,
     "status": "ok",
     "timestamp": 1601569241571,
     "user": {
      "displayName": "Samah Anemiche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji0DTjo2tp998fKQnRGzcFOAI6CDc0uLFY7rYU=s64",
      "userId": "17890150816051632495"
     },
     "user_tz": -120
    },
    "id": "DXTkkjrsIk7M",
    "outputId": "212d3091-b9a4-47cb-f316-a12c7c9e92d6"
   },
   "outputs": [],
   "source": [
    "! pip install paho-mqtt==1.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 35431,
     "status": "ok",
     "timestamp": 1601569247305,
     "user": {
      "displayName": "Samah Anemiche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji0DTjo2tp998fKQnRGzcFOAI6CDc0uLFY7rYU=s64",
      "userId": "17890150816051632495"
     },
     "user_tz": -120
    },
    "id": "d-DMnFSOIk7R"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "import logging\n",
    "\n",
    "from transformers import ElectraTokenizer, ElectraModel\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, LayerNorm\n",
    "from torchnlp.metrics import get_accuracy, get_token_accuracy\n",
    "from pytorch_lightning.metrics.functional import f1_score\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "import logging\n",
    "from pprint import pformat\n",
    "from itertools import chain\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 107910,
     "status": "ok",
     "timestamp": 1601569319962,
     "user": {
      "displayName": "Samah Anemiche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji0DTjo2tp998fKQnRGzcFOAI6CDc0uLFY7rYU=s64",
      "userId": "17890150816051632495"
     },
     "user_tz": -120
    },
    "id": "txQPSBwNIk7Y",
    "outputId": "0de3fe1a-fe52-47ee-d7d4-fd0616884bd4"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 59676,
     "status": "ok",
     "timestamp": 1601569391604,
     "user": {
      "displayName": "Samah Anemiche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji0DTjo2tp998fKQnRGzcFOAI6CDc0uLFY7rYU=s64",
      "userId": "17890150816051632495"
     },
     "user_tz": -120
    },
    "id": "-mxQzRnAIk7d",
    "outputId": "97ab93e8-a36a-4c92-f3d9-5634e4742817"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/My Drive/Chatbot/runs/DialoGPT\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"/content/drive/My Drive/Chatbot/runs/DialoGPT\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1371,
     "status": "ok",
     "timestamp": 1601569396176,
     "user": {
      "displayName": "Samah Anemiche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji0DTjo2tp998fKQnRGzcFOAI6CDc0uLFY7rYU=s64",
      "userId": "17890150816051632495"
     },
     "user_tz": -120
    },
    "id": "-K_FnUQ2Ik7h"
   },
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"[cls]\", \"<pad>\"]\n",
    "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<bos>', 'eos_token': '<eos>', 'cls_token': '[CLS]', 'pad_token': '<pad>', \n",
    "                         'additional_special_tokens': ['<speaker1>', '<speaker2>', 'anythingElse', 'none', 'greet', 'nothingElse', 'listProjects', 'helpYou', 'listTasks', 'thanks', 'onIt'],\n",
    "                        }\n",
    "\n",
    "orig_num_tokens = len(tokenizer.encoder)\n",
    "num_added_tokens = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN) # doesn't add if they are already there\n",
    "if num_added_tokens > 0:\n",
    "    model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4715,
     "status": "ok",
     "timestamp": 1601569403777,
     "user": {
      "displayName": "Samah Anemiche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji0DTjo2tp998fKQnRGzcFOAI6CDc0uLFY7rYU=s64",
      "userId": "17890150816051632495"
     },
     "user_tz": -120
    },
    "id": "v4s89NOsIk7m"
   },
   "outputs": [],
   "source": [
    "class RasaIntentEntityDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, markdown_lines: List[str], tokenizer, seq_len=128,):\n",
    "        self.intent_dict = {}\n",
    "        self.entity_dict = {}\n",
    "        self.entity_dict[\"O\"] = 0  # using BIO tagging\n",
    "\n",
    "        self.dataset = []\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        intent_value_list = []\n",
    "        entity_type_list = []\n",
    "\n",
    "        current_intent_focus = \"\"\n",
    "\n",
    "        text_list = []\n",
    "\n",
    "        for line in tqdm(markdown_lines,desc=\"Organizing Intent & Entity dictionary in NLU markdown file ...\",):\n",
    "            if len(line.strip()) < 2:\n",
    "                current_intent_focus = \"\"\n",
    "                continue\n",
    "\n",
    "            if \"## \" in line:\n",
    "                if \"intent:\" in line:\n",
    "                    intent_value_list.append(line.split(\":\")[1].strip())\n",
    "                    current_intent_focus = line.split(\":\")[1].strip()\n",
    "                else:\n",
    "                    current_intent_focus = \"\"\n",
    "\n",
    "            else:\n",
    "                if current_intent_focus != \"\":\n",
    "                    text = line[2:].strip().lower()\n",
    "\n",
    "                    for type_str in re.finditer(r\"\\([a-zA-Z_1-2]+\\)\", text):\n",
    "                        entity_type = (text[type_str.start() + 1 : type_str.end() - 1].replace(\"(\", \"\").replace(\")\", \"\"))\n",
    "                        entity_type_list.append(entity_type)\n",
    "\n",
    "                    text = re.sub(r\"\\([a-zA-Z_1-2]+\\)\", \"\", text)  # remove (...) str\n",
    "                    text = text.replace(\"[\", \"\").replace(\"]\", \"\")  # remove '[',']' special char\n",
    "\n",
    "                    if len(text) > 0:\n",
    "                        text_list.append(text.strip())\n",
    "\n",
    "\n",
    "        #dataset tokenizer setting\n",
    "        if \"ElectraTokenizer\" in str(type(tokenizer)):\n",
    "            self.tokenizer = tokenizer\n",
    "            self.pad_token_id = 0\n",
    "            self.unk_token_id = 1\n",
    "            self.eos_token_id = 3 #[SEP] token\n",
    "            self.bos_token_id = 2 #[CLS] token\n",
    "\n",
    "        else:\n",
    "            raise ValueError('not supported tokenizer type')\n",
    "\n",
    "        intent_value_list = sorted(intent_value_list)\n",
    "        for intent_value in intent_value_list:\n",
    "            if intent_value not in self.intent_dict.keys():\n",
    "                self.intent_dict[intent_value] = len(self.intent_dict)\n",
    "\n",
    "        entity_type_list = sorted(entity_type_list)\n",
    "        for entity_type in entity_type_list:\n",
    "            if entity_type + '_B' not in self.entity_dict.keys():\n",
    "                self.entity_dict[str(entity_type) + '_B'] = len(self.entity_dict)\n",
    "            if entity_type + '_I' not in self.entity_dict.keys():\n",
    "                self.entity_dict[str(entity_type) + '_I'] = len(self.entity_dict)\n",
    "\n",
    "        current_intent_focus = \"\"\n",
    "\n",
    "        for line in tqdm(markdown_lines, desc=\"Extracting Intent & Entity in NLU markdown files...\",):\n",
    "            if len(line.strip()) < 2:\n",
    "                current_intent_focus = \"\"\n",
    "                continue\n",
    "\n",
    "            if \"## \" in line:\n",
    "                if \"intent:\" in line:\n",
    "                    current_intent_focus = line.split(\":\")[1].strip()\n",
    "                else:\n",
    "                    current_intent_focus = \"\"\n",
    "            else:\n",
    "                if current_intent_focus != \"\":  # intent & entity sentence occur case\n",
    "                    text = line[2:].strip().lower()\n",
    "\n",
    "                    entity_value_list = []\n",
    "                    for value in re.finditer(r\"\\[(.*?)\\]\", text):\n",
    "                        entity_value_list.append(text[value.start() + 1 : value.end() - 1].replace(\"[\",\"\").replace(\"]\",\"\"))\n",
    "\n",
    "                    entity_type_list = []\n",
    "                    for type_str in re.finditer(r\"\\([a-zA-Z_1-2]+\\)\", text):\n",
    "                        entity_type = (text[type_str.start() + 1 : type_str.end() - 1].replace(\"(\",\"\").replace(\")\",\"\"))\n",
    "                        entity_type_list.append(entity_type)\n",
    "\n",
    "                    text = re.sub(r\"\\([a-zA-Z_1-2]+\\)\", \"\", text)  # remove (...) str\n",
    "                    text = text.replace(\"[\", \"\").replace(\"]\", \"\")  # remove '[',']' special char\n",
    "\n",
    "                    if len(text) > 0:\n",
    "                        each_data_dict = {}\n",
    "                        each_data_dict[\"text\"] = text.strip()\n",
    "                        each_data_dict[\"intent\"] = current_intent_focus\n",
    "                        each_data_dict[\"intent_idx\"] = self.intent_dict[current_intent_focus]\n",
    "                        each_data_dict[\"entities\"] = []\n",
    "\n",
    "                        for value, type_str in zip(entity_value_list, entity_type_list):\n",
    "                            for entity in re.finditer(value, text):\n",
    "                                entity_tokens = self.tokenize(value)\n",
    "\n",
    "                                for i, entity_token in enumerate(entity_tokens):\n",
    "                                    if i == 0:\n",
    "                                        BIO_type_str = type_str + '_B'\n",
    "                                    else:\n",
    "                                        BIO_type_str = type_str + '_I'\n",
    "\n",
    "                                    each_data_dict[\"entities\"].append(\n",
    "                                        {\n",
    "                                            \"start\": text.find(entity_token, entity.start(), entity.end()),\n",
    "                                            \"end\": text.find(entity_token, entity.start(), entity.end()) + len(entity_token),\n",
    "                                            \"entity\": type_str,\n",
    "                                            \"value\": entity_token,\n",
    "                                            \"entity_idx\": self.entity_dict[BIO_type_str],\n",
    "                                        }\n",
    "                                    )\n",
    "\n",
    "\n",
    "                        self.dataset.append(each_data_dict)\n",
    "\n",
    "        \n",
    "        print(f\"Intents: {self.intent_dict}\")\n",
    "        print(f\"Entities: {self.entity_dict}\")\n",
    "\n",
    "    def tokenize(self, text: str, skip_special_char=True):\n",
    "        if \"ElectraTokenizer\" in str(type(self.tokenizer)):\n",
    "            if skip_special_char:\n",
    "                return self.tokenizer.tokenize(text)\n",
    "            else:\n",
    "                return [token.replace('#','') for token in self.tokenizer.tokenize(text)]\n",
    "        else:\n",
    "            raise ValueError('not supported tokenizer type')\n",
    "            \n",
    "    def encode(self, text: str, padding: bool = True, return_tensor: bool = True):\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        if type(tokens) == list:\n",
    "            tokens = torch.tensor(tokens).long()\n",
    "        else:\n",
    "            tokens = tokens.long()\n",
    "\n",
    "        if padding:\n",
    "            if len(tokens) >= self.seq_len:\n",
    "                tokens = tokens[: self.seq_len]\n",
    "            else:\n",
    "                pad_tensor = torch.tensor([self.pad_token_id] * (self.seq_len - len(tokens)))\n",
    "            \n",
    "                tokens = torch.cat((tokens, pad_tensor), 0)\n",
    "\n",
    "        if return_tensor:\n",
    "            return tokens\n",
    "        else:\n",
    "            return tokens.numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.encode(self.dataset[idx][\"text\"])\n",
    "\n",
    "        intent_idx = torch.tensor([self.dataset[idx][\"intent_idx\"]])\n",
    "\n",
    "        entity_idx = np.array(self.seq_len * [0]) # O tag indicate 0(zero)\n",
    "\n",
    "        for entity_info in self.dataset[idx][\"entities\"]:\n",
    "            if \"ElectraTokenizer\" in str(type(self.tokenizer)):\n",
    "                ##check whether entity value is include in splitted token\n",
    "                for token_seq, token_value in enumerate(tokens):\n",
    "                    # Consider [CLS](bos) token\n",
    "                    if token_seq == 0:\n",
    "                        continue\n",
    "\n",
    "                    for entity_seq, entity_info in enumerate(self.dataset[idx][\"entities\"]):\n",
    "                        if (self.tokenizer.convert_ids_to_tokens([token_value.item()])[0] in entity_info[\"value\"]):\n",
    "                            entity_idx[token_seq] = entity_info[\"entity_idx\"]\n",
    "                            break\n",
    "\n",
    "        entity_idx = torch.from_numpy(entity_idx)\n",
    "\n",
    "        return tokens, intent_idx, entity_idx, self.dataset[idx][\"text\"]\n",
    "\n",
    "    def get_intent_idx(self):\n",
    "        return self.intent_dict\n",
    "\n",
    "    def get_entity_idx(self):\n",
    "        return self.entity_dict\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.tokenizer.vocab_size\n",
    "\n",
    "    def get_seq_len(self):\n",
    "        return self.seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3536,
     "status": "ok",
     "timestamp": 1601569403785,
     "user": {
      "displayName": "Samah Anemiche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji0DTjo2tp998fKQnRGzcFOAI6CDc0uLFY7rYU=s64",
      "userId": "17890150816051632495"
     },
     "user_tz": -120
    },
    "id": "sb67Gr4QIk7r"
   },
   "outputs": [],
   "source": [
    "class EmbeddingTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: None,\n",
    "        vocab_size: int,\n",
    "        seq_len: int,\n",
    "        intent_class_num: int,\n",
    "        entity_class_num: int,\n",
    "        d_model=512,\n",
    "        nhead=8,\n",
    "        num_encoder_layers=6,\n",
    "        dim_feedforward=2048,\n",
    "        dropout=0.1,\n",
    "        activation=\"relu\",\n",
    "        pad_token_id: int = 0,\n",
    "    ):\n",
    "        super(EmbeddingTransformer, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.seq_len = seq_len\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "        if backbone is None:\n",
    "            self.encoder = nn.TransformerEncoder(\n",
    "                TransformerEncoderLayer(\n",
    "                    d_model, nhead, dim_feedforward, dropout, activation\n",
    "                ),\n",
    "                num_encoder_layers,\n",
    "                LayerNorm(d_model),\n",
    "            )\n",
    "        else:  # pre-defined model architecture use\n",
    "            if backbone == \"electra\":\n",
    "                self.encoder = ElectraModel.from_pretrained(\"google/electra-small-discriminator\")\n",
    "\n",
    "            d_model = self.encoder.config.hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(self.seq_len, d_model)\n",
    "\n",
    "        self.intent_feature = nn.Linear(d_model, intent_class_num)\n",
    "        self.entity_feature = nn.Linear(d_model, entity_class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.backbone in [\"electra\"]:\n",
    "            feature = self.encoder(x)\n",
    "\n",
    "            if type(feature) == tuple:\n",
    "                feature = feature[0]  # last_hidden_state (N,S,E)\n",
    "\n",
    "            # first token in sequence used to intent classification\n",
    "            intent_feature = self.intent_feature(feature[:, 0, :]) # (N,E) -> (N,i_C)\n",
    "\n",
    "            # other tokens in sequence used to entity classification\n",
    "            entity_feature = self.entity_feature(feature[:, :, :]) # (N,S,E) -> (N,S,e_C)\n",
    "\n",
    "            return intent_feature, entity_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2722,
     "status": "ok",
     "timestamp": 1601569403792,
     "user": {
      "displayName": "Samah Anemiche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji0DTjo2tp998fKQnRGzcFOAI6CDc0uLFY7rYU=s64",
      "userId": "17890150816051632495"
     },
     "user_tz": -120
    },
    "id": "8U8pA5seIk7w"
   },
   "outputs": [],
   "source": [
    "class DualIntentEntityTransformer(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hparams = hparams\n",
    "        if type(self.hparams) == dict:\n",
    "            self.hparams = Namespace(**self.hparams)\n",
    "\n",
    "        self.dataset = RasaIntentEntityDataset(\n",
    "            markdown_lines=self.hparams.nlu_data,\n",
    "            tokenizer=self.hparams.tokenizer,\n",
    "        )\n",
    "\n",
    "        self.model = EmbeddingTransformer(\n",
    "            backbone=self.hparams.backbone,\n",
    "            vocab_size=self.dataset.get_vocab_size(),\n",
    "            seq_len=self.dataset.get_seq_len(),\n",
    "            intent_class_num=len(self.dataset.get_intent_idx()),\n",
    "            entity_class_num=len(self.dataset.get_entity_idx()),\n",
    "            d_model=self.hparams.d_model,\n",
    "            num_encoder_layers=self.hparams.num_encoder_layers,\n",
    "            pad_token_id=self.dataset.pad_token_id\n",
    "        )\n",
    "\n",
    "        self.train_ratio = self.hparams.train_ratio\n",
    "        self.batch_size = self.hparams.batch_size\n",
    "        self.optimizer = self.hparams.optimizer\n",
    "        self.intent_optimizer_lr = self.hparams.intent_optimizer_lr\n",
    "        self.entity_optimizer_lr = self.hparams.entity_optimizer_lr\n",
    "\n",
    "        self.intent_loss_fn = nn.CrossEntropyLoss()\n",
    "        # reduce O tag class weight to figure out entity imbalance distribution\n",
    "        self.entity_loss_fn = nn.CrossEntropyLoss(weight=torch.Tensor([0.1] + [1.0] * (len(self.dataset.get_entity_idx()) - 1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        train_length = int(len(self.dataset) * self.train_ratio)\n",
    "\n",
    "        self.train_dataset, self.val_dataset = random_split(\n",
    "            self.dataset, [train_length, len(self.dataset) - train_length],\n",
    "        )\n",
    "\n",
    "        self.hparams.intent_label = self.get_intent_label()\n",
    "        self.hparams.entity_label = self.get_entity_label()\n",
    "    \n",
    "    def get_intent_label(self):\n",
    "        self.intent_dict = {}\n",
    "        for k, v in self.dataset.intent_dict.items():\n",
    "            self.intent_dict[str(v)] = k\n",
    "        return self.intent_dict \n",
    "    \n",
    "    def get_entity_label(self):\n",
    "        self.entity_dict = {}\n",
    "        for k, v in self.dataset.entity_dict.items():\n",
    "            self.entity_dict[str(v)] = k\n",
    "        return self.entity_dict\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "        return val_loader\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        intent_optimizer = eval(\n",
    "            f\"{self.optimizer}(self.parameters(), lr={self.intent_optimizer_lr})\"\n",
    "        )\n",
    "        entity_optimizer = eval(\n",
    "            f\"{self.optimizer}(self.parameters(), lr={self.entity_optimizer_lr})\"\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            [intent_optimizer, entity_optimizer],\n",
    "            # [StepLR(intent_optimizer, step_size=1),StepLR(entity_optimizer, step_size=1),],\n",
    "            [\n",
    "                ReduceLROnPlateau(intent_optimizer, patience=1),\n",
    "                ReduceLROnPlateau(entity_optimizer, patience=1),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        self.model.train()\n",
    "\n",
    "        tokens, intent_idx, entity_idx, text = batch\n",
    "        intent_pred, entity_pred = self.forward(tokens)\n",
    "\n",
    "        intent_acc = get_accuracy(intent_pred.argmax(1).cpu(), intent_idx.cpu())[0]\n",
    "        #entity_acc = get_token_accuracy(entity_pred.argmax(2), entity_idx, ignore_index=self.dataset.pad_token_id)[0]\n",
    "\n",
    "        tensorboard_logs = {\n",
    "            \"train/intent/acc\": intent_acc,\n",
    "            #\"train/entity/acc\": entity_acc,\n",
    "        }\n",
    "\n",
    "        if optimizer_idx == 0:\n",
    "            intent_loss = self.intent_loss_fn(intent_pred, intent_idx.squeeze(1))\n",
    "            tensorboard_logs[\"train/intent/loss\"] = intent_loss\n",
    "\n",
    "            return {\n",
    "                \"loss\": intent_loss,\n",
    "                \"log\": tensorboard_logs,\n",
    "            }\n",
    "\n",
    "        if optimizer_idx == 1:\n",
    "            entity_loss = self.entity_loss_fn(entity_pred.transpose(1, 2), entity_idx.long(),)\n",
    "            tensorboard_logs[\"train/entity/loss\"] = entity_loss\n",
    "\n",
    "            return {\n",
    "                \"loss\": entity_loss,\n",
    "                \"log\": tensorboard_logs,\n",
    "            }\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.model.eval()\n",
    "\n",
    "        tokens, intent_idx, entity_idx, text = batch\n",
    "        intent_pred, entity_pred = self.forward(tokens)\n",
    "        \n",
    "\n",
    "        intent_acc = get_accuracy(intent_pred.argmax(1).cpu(), intent_idx.cpu())[0]\n",
    "        #entity_acc = get_token_accuracy(entity_pred.argmax(2), entity_idx, ignore_index=self.dataset.pad_token_id)[0]\n",
    "        intent_f1 = f1_score(intent_pred.argmax(1), intent_idx)\n",
    "\n",
    "        intent_loss = self.intent_loss_fn(intent_pred, intent_idx.squeeze(1))\n",
    "        entity_loss = self.entity_loss_fn(entity_pred.transpose(1, 2), entity_idx.long(),)\n",
    "\n",
    "        return {\n",
    "            \"val_intent_acc\": torch.Tensor([intent_acc]),\n",
    "            #\"val_entity_acc\": torch.Tensor([entity_acc]),\n",
    "            \"val_intent_f1\": intent_f1,\n",
    "            \"val_intent_loss\": intent_loss,\n",
    "            \"val_entity_loss\": entity_loss,\n",
    "            \"val_loss\": intent_loss + entity_loss,\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        avg_intent_acc = torch.stack([x[\"val_intent_acc\"] for x in outputs]).mean()\n",
    "        #avg_entity_acc = torch.stack([x[\"val_entity_acc\"] for x in outputs]).mean()\n",
    "        avg_intent_f1 = torch.stack([x[\"val_intent_f1\"] for x in outputs]).mean()\n",
    "\n",
    "        tensorboard_logs = {\n",
    "            \"val/loss\": avg_loss,\n",
    "            \"val/intent_acc\": avg_intent_acc,\n",
    "            #\"val/entity_acc\": avg_entity_acc,\n",
    "            \"val/intent_f1\": avg_intent_f1,\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"val_loss\": avg_loss,\n",
    "            \"val_intent_f1\": avg_intent_f1,\n",
    "            #\"val_entity_acc\": avg_entity_acc,\n",
    "            \"log\": tensorboard_logs,\n",
    "            \"progress_bar\": tensorboard_logs,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2991,
     "status": "ok",
     "timestamp": 1601569404832,
     "user": {
      "displayName": "Samah Anemiche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji0DTjo2tp998fKQnRGzcFOAI6CDc0uLFY7rYU=s64",
      "userId": "17890150816051632495"
     },
     "user_tz": -120
    },
    "id": "tRXEd1LkIk71"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "intent_dict = {}\n",
    "entity_dict = {}\n",
    "\n",
    "class Inferencer:\n",
    "    def __init__(self, checkpoint_path: str):\n",
    "        self.model = DualIntentEntityTransformer.load_from_checkpoint(checkpoint_path)\n",
    "        self.model.model.eval()\n",
    "\n",
    "        self.intent_dict = {}\n",
    "        for k, v in self.model.dataset.intent_dict.items():\n",
    "            self.intent_dict[v] = k # str key -> int key\n",
    "\n",
    "        self.entity_dict = {}\n",
    "        for k, v in self.model.dataset.entity_dict.items():\n",
    "            self.entity_dict[v] = k # str key -> int key\n",
    "\n",
    "        logging.info(\"intent dictionary\")\n",
    "        logging.info(self.intent_dict)\n",
    "        print()\n",
    "\n",
    "        logging.info(\"entity dictionary\")\n",
    "        logging.info(self.entity_dict)\n",
    "\n",
    "    def is_same_entity(self, i, j):\n",
    "        # check whether XXX_B, XXX_I tag are same \n",
    "        return self.entity_dict[i][:self.entity_dict[i].rfind('_')].strip() == self.entity_dict[j][:self.entity_dict[j].rfind('_')].strip()\n",
    "\n",
    "    def inference(self, text: str, intent_topk=5):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\n",
    "                \"model is not loaded, first call load_model(checkpoint_path)\"\n",
    "            )\n",
    "\n",
    "        text = text.strip().lower()\n",
    "\n",
    "        # encode text to token_indices\n",
    "        tokens = self.model.dataset.encode(text)\n",
    "        intent_result, entity_result = self.model.forward(tokens.unsqueeze(0).cpu())\n",
    "\n",
    "        # mapping intent result\n",
    "        rank_values, rank_indicies = torch.topk(\n",
    "            nn.Softmax(dim=1)(intent_result)[0], k=intent_topk\n",
    "        )\n",
    "        intent = {}\n",
    "        intent_ranking = []\n",
    "        for i, (value, index) in enumerate(\n",
    "            list(zip(rank_values.tolist(), rank_indicies.tolist()))\n",
    "        ):\n",
    "            intent_ranking.append(\n",
    "                {\"confidence\": value, \"name\": self.intent_dict[index]}\n",
    "            )\n",
    "\n",
    "            if i == 0:\n",
    "                intent[\"name\"] = self.intent_dict[index]\n",
    "                intent[\"confidence\"] = value\n",
    "\n",
    "        # mapping entity result\n",
    "        entities = []\n",
    "\n",
    "        # except first & last sequnce token whcih indicate BOS or [CLS] token & EOS or [SEP] token\n",
    "        _, entity_indices = torch.max((entity_result)[0][1:-1, :], dim=1)\n",
    "        start_idx = -1\n",
    "\n",
    "        #print ('tokens')\n",
    "        #print (tokens)\n",
    "        #print ('predicted entities')\n",
    "        #print (entity_indices)\n",
    "\n",
    "        entity_indices = entity_indices.tolist()[:len(text)]\n",
    "        start_token_position = -1\n",
    "\n",
    "        # except first sequnce token whcih indicate BOS or [CLS] token\n",
    "        if type(tokens) == torch.Tensor:\n",
    "            tokens = tokens.long().tolist()\n",
    "\n",
    "        for i, entity_idx_value in enumerate(entity_indices):\n",
    "            if entity_idx_value != 0 and start_token_position == -1:\n",
    "                start_token_position = i\n",
    "            elif start_token_position >= 0 and not self.is_same_entity(entity_indices[i-1],entity_indices[i]):\n",
    "                end_token_position = i - 1\n",
    "\n",
    "                #print ('start_token_position')\n",
    "                #print (start_token_position)\n",
    "                #print ('end_token_position')\n",
    "                #print (end_token_position)\n",
    "\n",
    "                # find start text position\n",
    "                token_idx = tokens[start_token_position + 1]\n",
    "                if \"ElectraTokenizer\" in str(\n",
    "                    type(self.model.dataset.tokenizer)\n",
    "                ):  # ElectraTokenizer\n",
    "                    token_value = self.model.dataset.tokenizer.convert_ids_to_tokens([token_idx])[0].replace(\"#\", \"\")\n",
    "\n",
    "                if len(token_value.strip()) == 0:\n",
    "                    start_token_position = -1\n",
    "                    continue\n",
    "                  \n",
    "                start_position = text.find(token_value.strip())\n",
    "\n",
    "                # find end text position\n",
    "                token_idx = tokens[end_token_position + 1]\n",
    "                if \"ElectraTokenizer\" in str(\n",
    "                    type(self.model.dataset.tokenizer)\n",
    "                ):  # ElectraTokenizer\n",
    "                    token_value = self.model.dataset.tokenizer.convert_ids_to_tokens([token_idx])[0].replace(\"#\", \"\")\n",
    "\n",
    "                end_position = text.find(token_value.strip(), start_position) + len(token_value.strip())\n",
    "\n",
    "                if self.entity_dict[entity_indices[i-1]] != \"O\": # ignore 'O' tag\n",
    "                    entities.append(\n",
    "                         {\n",
    "                            \"start\": start_position,\n",
    "                            \"end\": end_position,\n",
    "                            \"value\": text[start_position:end_position],\n",
    "                            \"entity\": self.entity_dict[entity_indices[i-1]][:self.entity_dict[entity_indices[i-1]].rfind('_')]\n",
    "                        }\n",
    "                    )\n",
    "                      \n",
    "                    start_token_position = -1\n",
    "\n",
    "                if entity_idx_value == 0:\n",
    "                    start_token_position = -1\n",
    "\n",
    "        result = {\n",
    "            \"text\": text,\n",
    "            \"intent\": intent,\n",
    "            \"intent_ranking\": intent_ranking,\n",
    "            \"entities\": entities,\n",
    "        }\n",
    "\n",
    "        # print (result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1579,
     "status": "ok",
     "timestamp": 1601569404847,
     "user": {
      "displayName": "Samah Anemiche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji0DTjo2tp998fKQnRGzcFOAI6CDc0uLFY7rYU=s64",
      "userId": "17890150816051632495"
     },
     "user_tz": -120
    },
    "id": "BSIFv2IiIk76"
   },
   "outputs": [],
   "source": [
    "def build_inputs(history, reply, tokenizer, lm_labels=True, with_eos=True):\n",
    "    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply. \"\"\"\n",
    "    bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-2])\n",
    "    sequence = [[bos]] + history + [reply + ([eos] if with_eos else [])]\n",
    "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
    "    instance = {}\n",
    "    instance[\"input_ids\"] = list(chain(*sequence))\n",
    "    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
    "    #instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
    "    instance[\"lm_labels\"] = [-100] * len(instance[\"input_ids\"])\n",
    "    if lm_labels:\n",
    "        instance[\"lm_labels\"] = ([-100] * sum(len(s) for s in sequence[:-1])) + [-100] + sequence[-1][1:]\n",
    "    return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3958,
     "status": "ok",
     "timestamp": 1601569408243,
     "user": {
      "displayName": "Samah Anemiche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji0DTjo2tp998fKQnRGzcFOAI6CDc0uLFY7rYU=s64",
      "userId": "17890150816051632495"
     },
     "user_tz": -120
    },
    "id": "Xf2RhmhHIk7_"
   },
   "outputs": [],
   "source": [
    "def top_filtering(logits, top_k=0., top_p=0.9, threshold=-float('Inf'), filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k, top-p (nucleus) and/or threshold filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k: <=0: no filtering, >0: keep only top k tokens with highest probability.\n",
    "            top_p: <=0.0: no filtering, >0.0: keep only a subset S of candidates, where S is the smallest subset\n",
    "                whose total probability mass is greater than or equal to the threshold top_p.\n",
    "                In practice, we select the highest probability tokens whose cumulative probability mass exceeds\n",
    "                the threshold top_p.\n",
    "            threshold: a minimal threshold to keep logits\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # Only work for batch size 1 for now - could update but it would obfuscate a bit the code\n",
    "    top_k = min(top_k, logits.size(-1))\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token in the top-k tokens\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        # Compute cumulative probabilities of sorted tokens\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probabilities > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # Back to unsorted indices and set them to -infinity\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    indices_to_remove = logits < threshold\n",
    "    logits[indices_to_remove] = filter_value\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2549,
     "status": "ok",
     "timestamp": 1601569408250,
     "user": {
      "displayName": "Samah Anemiche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji0DTjo2tp998fKQnRGzcFOAI6CDc0uLFY7rYU=s64",
      "userId": "17890150816051632495"
     },
     "user_tz": -120
    },
    "id": "PPanEat4Ik8D"
   },
   "outputs": [],
   "source": [
    "def sample_sequence(history, tokenizer, model, current_output=None):\n",
    "    special_tokens_ids = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
    "    if current_output is None:\n",
    "        current_output = []\n",
    "    \n",
    "    min_length = 1\n",
    "    max_length = 20\n",
    "    temperature = 0.7\n",
    "    top_k = 0\n",
    "    top_p = 0.9\n",
    "    no_sample = False\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        instance = build_inputs(history, current_output, tokenizer, with_eos=False)\n",
    "\n",
    "        input_ids = torch.tensor(instance[\"input_ids\"], device=device).unsqueeze(0)\n",
    "        token_type_ids = torch.tensor(instance[\"token_type_ids\"], device=device).unsqueeze(0)\n",
    "\n",
    "        logits = model(input_ids, token_type_ids=token_type_ids)\n",
    "        if isinstance(logits, tuple):  # for gpt2 and maybe others\n",
    "            logits = logits[0]\n",
    "        logits = logits[0, -1, :] / temperature\n",
    "        logits = top_filtering(logits, top_k=top_k, top_p=top_p)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        prev = torch.topk(probs, 1)[1] if no_sample else torch.multinomial(probs, 1)\n",
    "        if i < min_length and prev.item() in special_tokens_ids:\n",
    "            while prev.item() in special_tokens_ids:\n",
    "                if probs.max().item() == 1:\n",
    "                    warnings.warn(\"Warning: model generating special token with probability 1.\")\n",
    "                    break  # avoid infinitely looping over special token\n",
    "                prev = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        if prev.item() in special_tokens_ids:\n",
    "            break\n",
    "        current_output.append(prev.item())\n",
    "\n",
    "    return current_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2241,
     "status": "ok",
     "timestamp": 1601569411174,
     "user": {
      "displayName": "Samah Anemiche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji0DTjo2tp998fKQnRGzcFOAI6CDc0uLFY7rYU=s64",
      "userId": "17890150816051632495"
     },
     "user_tz": -120
    },
    "id": "V82YGOQPIk8I"
   },
   "outputs": [],
   "source": [
    "import paho.mqtt.client as mqtt\n",
    "import time\n",
    "import json\n",
    "\n",
    "broker_address = \"test.mosquitto.org\"\n",
    "port = 1883"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210,
     "referenced_widgets": [
      "62775f94dea1401a9c57b42f9bc88b41",
      "430c6f393b874723b25e43fcf558d9dc",
      "2f06aad0b501475d8d39c40caeed7536",
      "f358e429e5c44104bb864a37f28ab1ba",
      "93801cede5c74506998c1d23a1a16a1b",
      "19358934c1594c91954f158d3c448f67",
      "f039b79c0c0d407490d71356fc581c5d",
      "289289dfd1d244ca8a2bd89a8e4b473b",
      "fe0687fbba264a3ab216606d8c929f72",
      "b8383c072c91479c980f6f9c502f3af2",
      "6050ea0f6f8b485e8abdf99ccdd6af74",
      "5aefb6ecefb54d259d5cb18af197110d",
      "8888d6c3d89143049bf7c8bf101ead73",
      "110c666f0f0545bbb3bed850cf6b48da",
      "b5c09d05dc40445d99f5aa0a5fe12181",
      "dfbe8da3992f4267a3b4b5ef672219a2"
     ]
    },
    "executionInfo": {
     "elapsed": 9370,
     "status": "ok",
     "timestamp": 1601569423662,
     "user": {
      "displayName": "Samah Anemiche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji0DTjo2tp998fKQnRGzcFOAI6CDc0uLFY7rYU=s64",
      "userId": "17890150816051632495"
     },
     "user_tz": -120
    },
    "id": "D6b7eLxP6xq6",
    "outputId": "cc791e50-3a7b-46ba-9768-545199c61d63"
   },
   "outputs": [],
   "source": [
    "inferencer = Inferencer(\"/content/drive/My Drive/IntentEntity/lightning_logs/version_0/checkpoints/epoch=19.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1283,
     "status": "ok",
     "timestamp": 1601573102941,
     "user": {
      "displayName": "Samah Anemiche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji0DTjo2tp998fKQnRGzcFOAI6CDc0uLFY7rYU=s64",
      "userId": "17890150816051632495"
     },
     "user_tz": -120
    },
    "id": "xA2eZbd7Ik8L"
   },
   "outputs": [],
   "source": [
    "#history = []\n",
    "\n",
    "\n",
    "def on_connect(client, userdata, flags, rc):\n",
    "    print(\"Connected with result code \"+str(rc))\n",
    "    #client.subscribe(\"testChat/question\")\n",
    "\n",
    "# The callback for when a PUBLISH message is received from the server.\n",
    "def on_message(client, userdata, message):\n",
    "    #print(message.topic+\" \"+str(message.payload))\n",
    "    global FLAG\n",
    "    global chat\n",
    "    history = []\n",
    "    if str(message.topic) != publishTopic:\n",
    "        user_input = str(message.payload.decode(\"utf-8\"))\n",
    "        print('user_input: ', user_input)\n",
    "        #print(str(message.topic), user_input)\n",
    "        if user_input == \"quit\":\n",
    "            FLAG = False\n",
    "        else:\n",
    "            print('1')\n",
    "            history.append(tokenizer.encode(user_input))\n",
    "            print('2')\n",
    "            with torch.no_grad():\n",
    "                print('3')\n",
    "                out_ids = sample_sequence(history, tokenizer, model)\n",
    "                print('4')\n",
    "            print('5')\n",
    "            history.append(out_ids)\n",
    "            print(history)\n",
    "            \n",
    "            intent_entity = inferencer.inference(user_input, intent_topk=5)\n",
    "            intent = intent_entity['intent']['name']\n",
    "            entities = []\n",
    "        \n",
    "            if len(intent_entity['entities'])>0:\n",
    "                for i in range(len(intent_entity['entities'])):\n",
    "                    entities[i] = {'entity': intent_entity['entities'][i]['entity'], 'value':intent_entity['entities'][i]['value']}\n",
    "                    print(entities[i])\n",
    "            history = history[-3:]\n",
    "            out_text = tokenizer.decode(out_ids, skip_special_tokens=True)\n",
    "            \n",
    "            data = {'intent': intent, 'entity': entities, 'reply': out_text}\n",
    "\n",
    "            client.publish(publishTopic, out_text)\n",
    "        \n",
    "\n",
    "def on_subscribe(client, userdata, mid, granted_qos):\n",
    "    print(\"Subscribed: \", str(mid), str(granted_qos))\n",
    "\n",
    "def on_unsubscribe(client, userdata, mid):\n",
    "    print(\"Unsubscribed: \", str(mid))\n",
    "    \n",
    "def on_disconnect(client, userdata, rc):\n",
    "    if (rc != 0):\n",
    "        print(\"Unexpected Disconnection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "executionInfo": {
     "elapsed": 179529,
     "status": "ok",
     "timestamp": 1601573283976,
     "user": {
      "displayName": "Samah Anemiche",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gji0DTjo2tp998fKQnRGzcFOAI6CDc0uLFY7rYU=s64",
      "userId": "17890150816051632495"
     },
     "user_tz": -120
    },
    "id": "w5i4hrwIIk8Q",
    "outputId": "1f905195-7f12-4123-87fd-80da2d9647ef"
   },
   "outputs": [],
   "source": [
    "client = mqtt.Client()\n",
    "client.on_subscribe = on_subscribe\n",
    "client.on_unsubscribe = on_unsubscribe\n",
    "client.on_connect = on_connect\n",
    "client.on_message = on_message\n",
    "client.connect(broker_address, port)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "publishTopic = 'Chat/answer'\n",
    "subscribeTopic = 'Chat/question'\n",
    "\n",
    "FLAG = True\n",
    "\n",
    "client.loop_start()\n",
    "client.subscribe(subscribeTopic, qos=2)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "chat = input(\"Enter Message: \")\n",
    "client.publish(publishTopic, chat, qos=2)\n",
    "\n",
    "while True:\n",
    "    if FLAG == False or chat == \"quit\":\n",
    "        break\n",
    "        \n",
    "client.disconnect()\n",
    "client.loop_stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-UUbRznIk8S"
   },
   "outputs": [],
   "source": [
    "\"\"\"def run():\n",
    "    inferencer = Inferencer(\"lightning_logs/version_35/checkpoints/epoch=19.ckpt\")\n",
    "    \n",
    "    history = []\n",
    "    while True:\n",
    "        user_input = input(\">>> \")\n",
    "        while not user_input:\n",
    "            print('Prompt should not be empty!')\n",
    "            user_input = input(\">>> \")\n",
    "        if user_input == 'quit':\n",
    "            break\n",
    "        history.append(tokenizer.encode(user_input))\n",
    "        with torch.no_grad():\n",
    "            out_ids = sample_sequence(history, tokenizer, model)\n",
    "        history.append(out_ids)\n",
    "        \n",
    "        intent_entity = inferencer.inference(user_input, intent_topk=5)\n",
    "        intent = intent_entity['intent']['name']\n",
    "        entities = []\n",
    "        \n",
    "        if len(intent_entity['entities'])>0:\n",
    "            for i in range(len(intent_entity['entities'])):\n",
    "                entities[i] = {'entity': intent_entity['entities'][i]['entity'], 'value':intent_entity['entities'][i]['value']}\n",
    "                print(entities[i])\n",
    "        \n",
    "        #history = history[-(2*2+1):]\n",
    "        ######Only the last 3\n",
    "        history = history[-3:]\n",
    "        out_text = tokenizer.decode(out_ids, skip_special_tokens=True)\n",
    "        print(out_text)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXPIwW3KIk8W"
   },
   "outputs": [],
   "source": [
    "#run()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Run_Colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "110c666f0f0545bbb3bed850cf6b48da": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19358934c1594c91954f158d3c448f67": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "289289dfd1d244ca8a2bd89a8e4b473b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f06aad0b501475d8d39c40caeed7536": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_19358934c1594c91954f158d3c448f67",
      "max": 466,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_93801cede5c74506998c1d23a1a16a1b",
      "value": 466
     }
    },
    "430c6f393b874723b25e43fcf558d9dc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5aefb6ecefb54d259d5cb18af197110d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dfbe8da3992f4267a3b4b5ef672219a2",
      "placeholder": "​",
      "style": "IPY_MODEL_b5c09d05dc40445d99f5aa0a5fe12181",
      "value": " 54.2M/54.2M [00:24&lt;00:00, 2.26MB/s]"
     }
    },
    "6050ea0f6f8b485e8abdf99ccdd6af74": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_110c666f0f0545bbb3bed850cf6b48da",
      "max": 54245363,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8888d6c3d89143049bf7c8bf101ead73",
      "value": 54245363
     }
    },
    "62775f94dea1401a9c57b42f9bc88b41": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2f06aad0b501475d8d39c40caeed7536",
       "IPY_MODEL_f358e429e5c44104bb864a37f28ab1ba"
      ],
      "layout": "IPY_MODEL_430c6f393b874723b25e43fcf558d9dc"
     }
    },
    "8888d6c3d89143049bf7c8bf101ead73": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "93801cede5c74506998c1d23a1a16a1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "b5c09d05dc40445d99f5aa0a5fe12181": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b8383c072c91479c980f6f9c502f3af2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dfbe8da3992f4267a3b4b5ef672219a2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f039b79c0c0d407490d71356fc581c5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f358e429e5c44104bb864a37f28ab1ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_289289dfd1d244ca8a2bd89a8e4b473b",
      "placeholder": "​",
      "style": "IPY_MODEL_f039b79c0c0d407490d71356fc581c5d",
      "value": " 466/466 [00:00&lt;00:00, 552B/s]"
     }
    },
    "fe0687fbba264a3ab216606d8c929f72": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6050ea0f6f8b485e8abdf99ccdd6af74",
       "IPY_MODEL_5aefb6ecefb54d259d5cb18af197110d"
      ],
      "layout": "IPY_MODEL_b8383c072c91479c980f6f9c502f3af2"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
